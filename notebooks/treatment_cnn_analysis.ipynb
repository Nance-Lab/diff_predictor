{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import normalize\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Dense,BatchNormalization,Conv2D, Conv1D\n",
    "from keras.layers import Input,GlobalMaxPooling2D,concatenate, GlobalMaxPooling1D, Flatten\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping,ReduceLROnPlateau,ModelCheckpoint\n",
    "from os import listdir, getcwd, chdir\n",
    "from os.path import isfile, join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workbookDir = getcwd()\n",
    "print('Current Notebook Dir: ' + workbookDir)\n",
    "chdir(workbookDir) # Go to current workbook Dir\"\n",
    "chdir('..')        # Go up one\n",
    "print(f'Using current directory for loading data: {getcwd()}')\n",
    "workbookDir = getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../diff_predictor/raw_data_pnn/'\n",
    "chabc_train_filelist = [f for f in listdir(data_path) if isfile(join(data_path, f)) and 'ChABC' in f and 'ChABC_brain_4' not in f]\n",
    "chabc_validation_filelist = [f for f in listdir(data_path) if isfile(join(data_path, f)) and 'ChABC_brain_4' in f]\n",
    "nt_train_filelist = [f for f in listdir(data_path) if isfile(join(data_path, f)) and 'NT' in f and 'NT_brain_4' not in f]\n",
    "nt_validation_filelist = [f for f in listdir(data_path) if isfile(join(data_path, f)) and 'NT_brain_4' in f]\n",
    "\n",
    "\n",
    "print(len(nt_validation_filelist)) #Check to make sure there are the right number of files, in this case 180 - 60 per class\n",
    "print(len(chabc_validation_filelist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filelist[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = np.zeros([64,1])\n",
    "label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = np.zeros([64,100-1,1])\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partition = {\n",
    "    'train': df['Track_ID'].unique()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partition = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_filelist = chabc_train_filelist + nt_train_filelist\n",
    "print(len(training_filelist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up training files and labels\n",
    "\n",
    "lengths = []\n",
    "\n",
    "track_ids = []\n",
    "training_labels = dict()\n",
    "for file in training_filelist:\n",
    "\n",
    "    df = pd.read_csv(data_path + file)\n",
    "\n",
    "    for track_id in df['Track_ID'].unique():\n",
    "        label = file[:-4] + f'_track_{int(track_id)}'\n",
    "        if \"NT\" in file:\n",
    "            training_labels[str(label)] = 1\n",
    "        elif \"ChABC\" in file:\n",
    "            training_labels[str(label)] = 0\n",
    "        \n",
    "        track_data = df[df['Track_ID']==track_id][['X', 'Y']]\n",
    "        track_data = track_data.fillna(0)\n",
    "    #     #dataset[str(track_id)] = np.array(track_data)\n",
    "\n",
    "        test_array = np.array(track_data)\n",
    "        #norm_array = normalize(test_array)\n",
    "        # print(np.count_nonzero(test_array))\n",
    "        # lengths.append(len(test_array))\n",
    "        #print(np.count_nonzero(np.isnan(test_array)))\n",
    "        if np.count_nonzero(test_array) >19:\n",
    "            print(np.count_nonzero(test_array))\n",
    "            track_ids.append(str(label))\n",
    "            np.save(str(f'data/treatment_training_data/{file}').replace(\".csv\", f\"_track_{int(track_id)}\"), test_array)\n",
    "\n",
    "partition['train'] = track_ids\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array_lengths = np.array(lengths)\n",
    "print(len(array_lengths))\n",
    "np.count_nonzero(array_lengths>50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_filelist = nt_validation_filelist + chabc_validation_filelist\n",
    "print(len(validation_filelist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up validation files and labels\n",
    "\n",
    "track_ids = []\n",
    "validation_labels = dict()\n",
    "for file in validation_filelist:\n",
    "\n",
    "    df = pd.read_csv(data_path + file)\n",
    "\n",
    "    for track_id in df['Track_ID'].unique():\n",
    "        label = file[:-4] + f'_track_{int(track_id)}'\n",
    "        if \"NT\" in file:\n",
    "            validation_labels[str(label)] = 1\n",
    "        else:\n",
    "            validation_labels[str(label)] = 0\n",
    "        \n",
    "        track_data = df[df['Track_ID']==track_id][['X', 'Y']]\n",
    "        track_data = track_data.fillna(0)\n",
    "    #     #dataset[str(track_id)] = np.array(track_data)\n",
    "        test_array = np.array(track_data)\n",
    "        #norm_array = normalize(test_array)\n",
    "        if np.count_nonzero(test_array) >19:\n",
    "            track_ids.append(str(label))\n",
    "            np.save(str(f'data/treatment_validation_data/{file}').replace(\".csv\", f\"_track_{int(track_id)}\"), test_array)\n",
    "\n",
    "partition['validation'] = track_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_array[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_data_path = '../diff_predictor/data/10div_training_data/'\n",
    "one_um_filelist = [f for f in listdir(cnn_data_path) if isfile(join(cnn_data_path, f)) and '1uM' in f]\n",
    "\n",
    "nt_filelist = [f for f in listdir(cnn_data_path) if isfile(join(cnn_data_path, f)) and 'NT' in f]\n",
    "len(one_um_filelist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = out[out!=0]\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(5, 5, figsize=(10,10))\n",
    "for i in range(5):\n",
    "    for j in range(5):\n",
    "        file = np.load(cnn_data_path+one_um_filelist[counter])\n",
    "        if np.count_nonzero(file) >0:\n",
    "            counter += 1\n",
    "            axes[i,j].plot(file[:,0], file[:,1])\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(5, 5, figsize=(10,10))\n",
    "for i in range(5):\n",
    "    for j in range(5):\n",
    "        file = np.load(cnn_data_path+nt_filelist[counter])\n",
    "        counter += 1\n",
    "        axes[i,j].plot(file[:,0], file[:,1])\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partition.keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[track_ids[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MptDataGenerator(keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, data_path, list_IDs, labels, batch_size=128, dim=(651,2), n_channels=None,\n",
    "                 n_classes=2, shuffle=True):\n",
    "        'Initialization'\n",
    "        self.dim = dim\n",
    "        self.batch_size = batch_size\n",
    "        self.labels = labels\n",
    "        self.list_IDs = list_IDs\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.shuffle = shuffle\n",
    "        self.data_path = data_path\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "        # Find list of IDs\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(list_IDs_temp)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.list_IDs))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "        # Initialization\n",
    "        X = np.empty((self.batch_size, *self.dim))#, self.n_channels))\n",
    "        y = np.empty((self.batch_size), dtype=int)\n",
    "\n",
    "        # Generate data\n",
    "        for i, ID in enumerate(list_IDs_temp):\n",
    "            # Store sample\n",
    "            out_array = np.load(self.data_path + ID + '.npy')\n",
    "            out_array = normalize(out_array)\n",
    "            X[i,] = out_array\n",
    "\n",
    "            # Store class\n",
    "            y[i] = self.labels[ID]\n",
    "\n",
    "        return X, keras.utils.to_categorical(y, num_classes=self.n_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_generator = MptDataGenerator('../diff_predictor/data/treatment_training_data/', partition['train'], training_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_generator = MptDataGenerator('../diff_predictor/data/treatment_validation_data/', partition['validation'], validation_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = Input((651, 2))\n",
    "initializer = 'random_normal'\n",
    "f = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = Conv1D(f,4,padding='causal',activation='relu',kernel_initializer=initializer)(inputs)\n",
    "x1 = BatchNormalization()(x1)\n",
    "x1 = Conv1D(f,4,dilation_rate=2,padding='causal',activation='relu',kernel_initializer=initializer)(x1)\n",
    "x1 = BatchNormalization()(x1)\n",
    "x1 = Conv1D(f,4,dilation_rate=4,padding='causal',activation='relu',kernel_initializer=initializer)(x1)\n",
    "x1 = BatchNormalization()(x1)\n",
    "x1 = GlobalMaxPooling1D()(x1)\n",
    "\n",
    "dense = Dense(512,activation='relu')(x1)\n",
    "dense = Dense(256,activation='relu')(dense)\n",
    "dense2 = Dense(2,activation='softmax')(dense)\n",
    "model = Model(inputs=inputs, outputs=dense2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = Conv1D(f,4,padding='causal',activation='relu',kernel_initializer=initializer)(inputs)\n",
    "x1 = BatchNormalization()(x1)\n",
    "x1 = Conv1D(f,4,dilation_rate=2,padding='causal',activation='relu',kernel_initializer=initializer)(x1)\n",
    "x1 = BatchNormalization()(x1)\n",
    "x1 = Conv1D(f,4,dilation_rate=4,padding='causal',activation='relu',kernel_initializer=initializer)(x1)\n",
    "x1 = BatchNormalization()(x1)\n",
    "x1 = GlobalMaxPooling1D()(x1)\n",
    "\n",
    "\n",
    "x2 = Conv1D(f,2,padding='causal',activation='relu',kernel_initializer=initializer)(inputs)\n",
    "x2 = BatchNormalization()(x2)\n",
    "x2 = Conv1D(f,2,dilation_rate=2,padding='causal',activation='relu',kernel_initializer=initializer)(x2)\n",
    "x2 = BatchNormalization()(x2)\n",
    "x2 = Conv1D(f,2,dilation_rate=4,padding='causal',activation='relu',kernel_initializer=initializer)(x2)\n",
    "x2 = BatchNormalization()(x2)\n",
    "x2 = GlobalMaxPooling1D()(x2)\n",
    "\n",
    "\n",
    "x3 = Conv1D(f,3,padding='causal',activation='relu',kernel_initializer=initializer)(inputs)\n",
    "x3 = BatchNormalization()(x3)\n",
    "x3 = Conv1D(f,3,dilation_rate=2,padding='causal',activation='relu',kernel_initializer=initializer)(x3)\n",
    "x3 = BatchNormalization()(x3)\n",
    "x3 = Conv1D(f,3,dilation_rate=4,padding='causal',activation='relu',kernel_initializer=initializer)(x3)\n",
    "x3 = BatchNormalization()(x3)\n",
    "x3 = GlobalMaxPooling1D()(x3)\n",
    "\n",
    "\n",
    "x4 = Conv1D(f,10,padding='causal',activation='relu',kernel_initializer=initializer)(inputs)\n",
    "x4 = BatchNormalization()(x4)\n",
    "x4 = Conv1D(f,10,dilation_rate=4,padding='causal',activation='relu',kernel_initializer=initializer)(x4)\n",
    "x4 = BatchNormalization()(x4)\n",
    "x4 = Conv1D(f,10,dilation_rate=8,padding='causal',activation='relu',kernel_initializer=initializer)(x4)\n",
    "x4 = BatchNormalization()(x4)\n",
    "x4 = GlobalMaxPooling1D()(x4)\n",
    "\n",
    "\n",
    "x5 = Conv1D(f,20,padding='same',activation='relu',kernel_initializer=initializer)(inputs)\n",
    "x5 = BatchNormalization()(x5)\n",
    "x5 = GlobalMaxPooling1D()(x5)\n",
    "\n",
    "\n",
    "con = concatenate([x1,x2,x3,x4,x5])\n",
    "dense = Dense(512,activation='relu')(con)\n",
    "dense = Dense(128,activation='relu')(dense)\n",
    "dense2 = Dense(2,activation='softmax')(dense)\n",
    "model = Model(inputs=inputs, outputs=dense2)\n",
    "\n",
    "optimizer = Adam(lr=1e-5)\n",
    "model.compile(optimizer=optimizer,loss='categorical_crossentropy',metrics=['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1000, input_dim=inputs, activation='relu'))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "# con = concatenate([x1,x2,x3,x4])\n",
    "# dense = Dense(512,activation='relu')(con)\n",
    "# dense = Dense(256#,activation='relu')(dense)\n",
    "# dense2 = Dense(1,activation='sigmoid')(dense)\n",
    "# model = Model(inputs=inputs, outputs=dense2)\n",
    "\n",
    "# optimizer = Adam(lr=1e-5)\n",
    "# model.compile(optimizer=optimizer,loss='mse',metrics=['mse'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='mse',\n",
    "              optimizer=Adam(lr=1e-5),\n",
    "              metrics=['mse', 'accuracy'])\n",
    "\n",
    "model.fit(training_generator, epochs=3, validation_data=validation_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.predict(validation_generator, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import scale\n",
    "output = scale()\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_y = np.array(df[df['Track_ID']== 1][['X', 'Y']])\n",
    "x_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm = np.linalg.norm(x_y, 1)\n",
    "im = x_y/norm\n",
    "plt.figure(figsize=(10,500))\n",
    "plt.imshow(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_y[:2,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_array(A):\n",
    "    return (A-np.min(A))/(np.max(A) - np.min(A))\n",
    "\n",
    "output = scale_array(x_y[:19,:])\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "%matplotlib inline\n",
    "im = np.array(output).astype('uint8')\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(im, cmap='Greys')\n",
    "# grayImage = cv2.cvtColor(uint_img, cv2.COLOR_GRAY2BGR)\n",
    "# cv2.imshow(mat=grayImage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def traj_generator(batch_size, datapath):\n",
    "    out = np.zeros([batch_size, 651, 2]) #initialize trajectory array\n",
    "    label = np.zeros([batch_size, 1]) #initialize label array\n",
    "    batch_count = 0\n",
    "    while True:\n",
    "        df = pd.read_csv(datapath)\n",
    "        for i in range(batch_size):\n",
    "            batch_count += 1\n",
    "            x_y = np.array(df[df['Track_ID']==i][['X', 'Y']])\n",
    "            out[i,:,:] = x_y\n",
    "            label[i] = 1\n",
    "        yield out, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = Input((651, 2))\n",
    "initializer = 'random_normal'\n",
    "f = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = Conv1D(f,4,padding='causal',activation='relu',kernel_initializer=initializer)(inputs)\n",
    "x1 = BatchNormalization()(x1)\n",
    "x1 = Conv1D(f,4,dilation_rate=2,padding='causal',activation='relu',kernel_initializer=initializer)(x1)\n",
    "x1 = BatchNormalization()(x1)\n",
    "x1 = Conv1D(f,4,dilation_rate=4,padding='causal',activation='relu',kernel_initializer=initializer)(x1)\n",
    "x1 = BatchNormalization()(x1)\n",
    "x1 = GlobalMaxPooling1D()(x1)\n",
    "\n",
    "\n",
    "x2 = Conv1D(f,2,padding='causal',activation='relu',kernel_initializer=initializer)(inputs)\n",
    "x2 = BatchNormalization()(x2)\n",
    "x2 = Conv1D(f,2,dilation_rate=2,padding='causal',activation='relu',kernel_initializer=initializer)(x2)\n",
    "x2 = BatchNormalization()(x2)\n",
    "x2 = Conv1D(f,2,dilation_rate=4,padding='causal',activation='relu',kernel_initializer=initializer)(x2)\n",
    "x2 = BatchNormalization()(x2)\n",
    "x2 = GlobalMaxPooling1D()(x2)\n",
    "\n",
    "\n",
    "x3 = Conv1D(f,3,padding='causal',activation='relu',kernel_initializer=initializer)(inputs)\n",
    "x3 = BatchNormalization()(x3)\n",
    "x3 = Conv1D(f,3,dilation_rate=2,padding='causal',activation='relu',kernel_initializer=initializer)(x3)\n",
    "x3 = BatchNormalization()(x3)\n",
    "x3 = Conv1D(f,3,dilation_rate=4,padding='causal',activation='relu',kernel_initializer=initializer)(x3)\n",
    "x3 = BatchNormalization()(x3)\n",
    "x3 = GlobalMaxPooling1D()(x3)\n",
    "\n",
    "\n",
    "x4 = Conv1D(f,10,padding='causal',activation='relu',kernel_initializer=initializer)(inputs)\n",
    "x4 = BatchNormalization()(x4)\n",
    "x4 = Conv1D(f,10,dilation_rate=5,padding='causal',activation='relu',kernel_initializer=initializer)(x4)\n",
    "x4 = BatchNormalization()(x4)\n",
    "x4 = Conv1D(f,10,dilation_rate=10,padding='causal',activation='relu',kernel_initializer=initializer)(x4)\n",
    "x4 = BatchNormalization()(x4)\n",
    "x4 = GlobalMaxPooling1D()(x4)\n",
    "\n",
    "con = concatenate([x1,x2,x3,x4])\n",
    "dense = Dense(512,activation='relu')(con)\n",
    "dense = Dense(256,activation='relu')(dense)\n",
    "dense2 = Dense(1,activation='sigmoid')(dense)\n",
    "model = Model(inputs=inputs, outputs=dense2)\n",
    "\n",
    "optimizer = Adam(lr=1e-5)\n",
    "model.compile(optimizer=optimizer,loss='mse',metrics=['mse', 'accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(16, input_dim=1, activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(1, activation='linear'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='mean_absolute_error',\n",
    "              optimizer=Adam(),\n",
    "              metrics=['mean_squared_error'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='mean_absolute_error',\n",
    "              optimizer=Adam(),\n",
    "              metrics=['mean_squared_error', 'accuracy'])\n",
    "\n",
    "model.fit(training_generator, epochs=2, validation_data=validation_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
  },
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
