{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# --- added to file ----\n",
        "# Takes in a String, \"bucket_name\", a string, \"remote_folder\",\n",
        "# and a list of strings or a single string, \"keywords\". Gets all\n",
        "# s3 keys for bucket_name/remote_folder. Uses a list convention\n",
        "# to go through keywords (i.e): ['a', 'b', 'c OR d OR e'] will \n",
        "# find all files containing 'a' and 'b' and either 'c', 'd', or 'e'.\n",
        "# Using '' will return every file key in folder.\n",
        "def get_s3_keys(bucket_name, remote_folder, keywords=''):\n",
        "    s3 = boto3.resource('s3')\n",
        "    bucket = s3.Bucket(bucket_name)\n",
        "    obj_list = []\n",
        "    keywords = [i.split('OR') for i in list(keywords)]\n",
        "    keywords = [list(map(lambda x:x.strip(), i)) for i in keywords]\n",
        "    for object in bucket.objects.all():\n",
        "        filename = object.key.split(\"/\")[-1]\n",
        "        kwds_in = all(any(k in filename for k in ([keyword]*isinstance(keyword, str) or keyword)) for keyword in keywords)\n",
        "        if remote_folder in object.key and kwds_in:\n",
        "            obj_list.append(s3.Object(object.bucket_name, object.key))\n",
        "    return obj_list"
      ],
      "outputs": [],
      "execution_count": 1,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from os import listdir, getcwd, chdir\n",
        "from os.path import isfile, join\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import csv"
      ],
      "outputs": [],
      "execution_count": 2,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/c/Users/david/Documents/nancework/source/diff_predictor/notebooks\n"
          ]
        }
      ],
      "execution_count": 3,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import csv\n",
        "\n",
        "# Takes in a path and list of keywords. Returns a list of filenames\n",
        "# that are within the path that contain one of the keyword in the list.\n",
        "# Set keyword to \"\" to get all files in the path.\n",
        "def get_files(path, keywords = [\"features_ OR msd_\"]):\n",
        "    \"\"\"\n",
        "    Takes in a path and list of keywords. Returns a list of filenames\n",
        "    that are within the path that contain one of the keyword in the list.\n",
        "    Set keyword to \"\" to get all files in the path.\n",
        "    \"\"\"\n",
        "    keywords = [i.split('OR') for i in list(keywords)]\n",
        "    keywords = [list(map(lambda x:x.strip(), i)) for i in keywords]\n",
        "    files = [f for f in listdir(path) if isfile(join(path, f))]\n",
        "    file_list = []\n",
        "    for filename in files:\n",
        "        kwds_in = all(any(k in filename for k in ([keyword]*isinstance(keyword, str) or keyword)) for keyword in keywords)\n",
        "        if (kwds_in):\n",
        "            file_list.append(filename)\n",
        "    return file_list\n",
        "\n",
        "# Pre: Both files must exhist; Feature must be in the feature file\n",
        "# Throws a FileNotFoundError exception if preconditions not met\n",
        "#\n",
        "# Adds a feature from produced features file to the track file.\n",
        "def combine_track(trackFile, feature=None, featureDF=None):\n",
        "    '''\n",
        "    Adds a feature or set of feature to the corresponding track file\n",
        "    Preconditions: Both files must exhist; Feature(s) must be in the \n",
        "    feature file. \n",
        "    \n",
        "    Input:\n",
        "    ------\n",
        "    trackFile : string :\n",
        "        The file location of the dataframe \n",
        "    feature : list : string : tuple :\n",
        "        feature or set of features to attach to track dataframe\n",
        "    Output:\n",
        "    -------\n",
        "    trackDF : pd.DataFrame :\n",
        "        DataFrame of the combined tracks\n",
        "    '''\n",
        "    if isinstance(trackFile, str):\n",
        "        try:\n",
        "            trackDF = pd.read_csv(trackFile)\n",
        "        except FileNotFoundError:\n",
        "            raise(\"DataFrame cannot be located\")\n",
        "    else:\n",
        "        trackDF = trackFile\n",
        "    if featureDF is None:\n",
        "        featureDF = find_pair(trackFile)\n",
        "    if feature is None:\n",
        "        feature = np.setdiff1d(featureDF.columns.values, trackDF.columns.values)\n",
        "    elif isinstance(feature, str):\n",
        "        feature = [feature]\n",
        "    elif isinstance(feature, tuple):\n",
        "        feature = list(feature)\n",
        "    trackDF = trackDF.reindex(columns=[*trackDF.columns.tolist()] + [*feature], fill_value=np.nan)\n",
        "    maxFrames = int(trackDF[\"Frame\"].max())\n",
        "    maxTracks = int(trackDF[\"Track_ID\"].max())\n",
        "    for i in range(int(maxTracks)+1):\n",
        "        for feat in feature:\n",
        "            trackFeature = featureDF.loc[i, feat]\n",
        "            trackDF.loc[(maxFrames)*(i+1) + i, feat] = trackFeature\n",
        "    return trackDF\n",
        "\n",
        "# Trys to find the feature file pair for either the msd_ or Traj_\n",
        "# Returns the pd.DataFrame of that pair if found.\n",
        "def find_pair(filename):\n",
        "    \"\"\"\n",
        "    Trys to find the feature file pair for either the msd_ or traj_ df,\n",
        "    or the Traj_ or msd_ file for input feauture_ file.\n",
        "    Returns the pd.DataFrame of that pair if found.\n",
        "    \"\"\"\n",
        "    if \"msd_\" in filename:\n",
        "        try:\n",
        "            filename = filename.replace(\"msd_\", \"\").replace(\"Traj_\", \"\")\n",
        "            filename = filename.split(\"/\")\n",
        "            filename[-1] = \"features_\" + filename[-1]\n",
        "            featureFile = \"/\".join(filename)\n",
        "            return pd.read_csv(featureFile)\n",
        "        except FileNotFoundError:\n",
        "            print(\"File pair could not be found\")  \n",
        "    elif \"features_\" in filename:\n",
        "        try:\n",
        "            filename = filename.replace(\"features_\", \"\")\n",
        "            filename = filename.split(\"/\")\n",
        "            filename[-1] = \"msd_\" + filename[-1]\n",
        "            featureFile = \"/\".join(filename)\n",
        "            return pd.read_csv(featureFile)\n",
        "        except:\n",
        "            try:\n",
        "                filename = filename.replace(\"features_\", \"\")\n",
        "                filename = filename.split(\"/\")\n",
        "                filename[-1] = \"Traj_\" + filename[-1]\n",
        "                featureFile = \"/\".join(filename)\n",
        "                return pd.read_csv(featureFile)\n",
        "            except FileNotFoundError:\n",
        "                print(\"File pair could not be found\")"
      ],
      "outputs": [],
      "execution_count": 4,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "if not 'workbookDir' in globals():\n",
        "    workbookDir = getcwd()\n",
        "print('Current Notebook Dir: ' + workbookDir)\n",
        "chdir(workbookDir) # Go to current workbook Dir\n",
        "chdir('..')        # Go up one\n",
        "workbookDir = getcwd()\n",
        "print(f'Using current directory for loading data: {getcwd()}')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current Notebook Dir: C:\\Users\\david\\Documents\\nancework\\source\\diff_predictor\\notebooks\n",
            "Using current directory for loading data: C:\\Users\\david\\Documents\\nancework\\source\\diff_predictor\n"
          ]
        }
      ],
      "execution_count": 5,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_path = './raw_data_region_cortex_striatum'\n",
        "track_file_list = get_files(dataset_path, keywords=['msd_'])\n",
        "feature_file_list = get_files(dataset_path, ['features_'])"
      ],
      "outputs": [],
      "execution_count": 6,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/c/Users/david/Documents/nancework/source/diff_predictor\n"
          ]
        }
      ],
      "execution_count": 7,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "feature_file_list"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 8,
          "data": {
            "text/plain": "['features_NT_slice_1_cortex_vid_10.csv',\n 'features_NT_slice_1_cortex_vid_6.csv',\n 'features_NT_slice_1_cortex_vid_7.csv',\n 'features_NT_slice_1_cortex_vid_8.csv',\n 'features_NT_slice_1_cortex_vid_9.csv',\n 'features_NT_slice_1_striatum_vid_1.csv',\n 'features_NT_slice_1_striatum_vid_2.csv',\n 'features_NT_slice_1_striatum_vid_3.csv',\n 'features_NT_slice_1_striatum_vid_4.csv',\n 'features_NT_slice_1_striatum_vid_5.csv',\n 'features_NT_slice_2_cortex_vid_1.csv',\n 'features_NT_slice_2_cortex_vid_2.csv',\n 'features_NT_slice_2_cortex_vid_3.csv',\n 'features_NT_slice_2_cortex_vid_4.csv',\n 'features_NT_slice_2_cortex_vid_5.csv',\n 'features_NT_slice_2_striatum_vid_1.csv',\n 'features_NT_slice_2_striatum_vid_2.csv',\n 'features_NT_slice_2_striatum_vid_3.csv',\n 'features_NT_slice_2_striatum_vid_4.csv',\n 'features_NT_slice_2_striatum_vid_5.csv']"
          },
          "metadata": {}
        }
      ],
      "execution_count": 8,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "fstats_tot = None\n",
        "video_num = 0\n",
        "for filename in feature_file_list:\n",
        "    try:\n",
        "        fstats = pd.read_csv(dataset_path + '/' + filename, encoding = \"ISO-8859-1\", index_col='Unnamed: 0')\n",
        "        tstats = find_pair(dataset_path + '/' + filename)\n",
        "        print('{} size: {}'.format(filename, fstats.shape))\n",
        "        if 'cortex' in filename:\n",
        "            fstats['region'] = pd.Series(fstats.shape[0]*[0], index=fstats.index)\n",
        "        elif 'striatum' in filename: \n",
        "            fstats['region'] = pd.Series(fstats.shape[0]*[1], index=fstats.index)\n",
        "        else:\n",
        "            print('Error, no target')\n",
        "        fstats['Video Number'] = pd.Series(fstats.shape[0]*[video_num], index=fstats.index)\n",
        "        fstats = combine_track(tstats, feature=np.append(feat, ['region']), featureDF=fstats)\n",
        "        if fstats_tot is None:\n",
        "            fstats_tot = fstats\n",
        "        else:\n",
        "            fstats_tot = fstats_tot.append(fstats, ignore_index=True)\n",
        "        video_num += 1\n",
        "    except Exception:\n",
        "        print('Skipped!: {}'.format(filename))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "features_NT_slice_1_cortex_vid_10.csv size: (4832, 23)\n",
            "features_NT_slice_1_cortex_vid_6.csv size: (7990, 23)\n",
            "features_NT_slice_1_cortex_vid_7.csv size: (4159, 23)\n",
            "features_NT_slice_1_cortex_vid_8.csv size: (1984, 23)\n",
            "features_NT_slice_1_cortex_vid_9.csv size: (6506, 23)\n",
            "features_NT_slice_1_striatum_vid_1.csv size: (2431, 23)\n",
            "features_NT_slice_1_striatum_vid_2.csv size: (2240, 23)\n",
            "features_NT_slice_1_striatum_vid_3.csv size: (1536, 23)\n",
            "features_NT_slice_1_striatum_vid_4.csv size: (2177, 23)\n",
            "features_NT_slice_1_striatum_vid_5.csv size: (2169, 23)\n",
            "features_NT_slice_2_cortex_vid_1.csv size: (1388, 23)\n",
            "features_NT_slice_2_cortex_vid_2.csv size: (1784, 23)\n",
            "features_NT_slice_2_cortex_vid_3.csv size: (3520, 23)\n",
            "features_NT_slice_2_cortex_vid_4.csv size: (1429, 23)\n",
            "features_NT_slice_2_cortex_vid_5.csv size: (2210, 23)\n",
            "features_NT_slice_2_striatum_vid_1.csv size: (8314, 23)\n",
            "features_NT_slice_2_striatum_vid_2.csv size: (10500, 23)\n",
            "features_NT_slice_2_striatum_vid_3.csv size: (11355, 23)\n",
            "features_NT_slice_2_striatum_vid_4.csv size: (10237, 23)\n",
            "features_NT_slice_2_striatum_vid_5.csv size: (13938, 23)\n",
            "Skipped!: features_NT_slice_2_striatum_vid_5.csv\n"
          ]
        }
      ],
      "execution_count": 8,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "fstats_tot"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 44,
          "data": {
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>Unnamed: 0.1</th>\n      <th>Frame</th>\n      <th>Gauss</th>\n      <th>MSDs</th>\n      <th>Mean_Intensity</th>\n      <th>Quality</th>\n      <th>SN_Ratio</th>\n      <th>Track_ID</th>\n      <th>X</th>\n      <th>...</th>\n      <th>asymmetry3</th>\n      <th>boundedness</th>\n      <th>efficiency</th>\n      <th>elongation</th>\n      <th>fractal_dim</th>\n      <th>frames</th>\n      <th>kurtosis</th>\n      <th>straightness</th>\n      <th>trappedness</th>\n      <th>region</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>288.015504</td>\n      <td>5.162109</td>\n      <td>0.701482</td>\n      <td>0.0</td>\n      <td>74.264553</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.753676</td>\n      <td>0.371693</td>\n      <td>288.348837</td>\n      <td>5.122284</td>\n      <td>0.643396</td>\n      <td>0.0</td>\n      <td>74.564264</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>2.0</td>\n      <td>2.0</td>\n      <td>0.797005</td>\n      <td>0.385012</td>\n      <td>290.767442</td>\n      <td>5.700928</td>\n      <td>0.706083</td>\n      <td>0.0</td>\n      <td>74.879671</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>3.0</td>\n      <td>3.0</td>\n      <td>0.863716</td>\n      <td>0.401320</td>\n      <td>289.759690</td>\n      <td>5.593384</td>\n      <td>0.756702</td>\n      <td>0.0</td>\n      <td>73.889012</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>4.0</td>\n      <td>4.0</td>\n      <td>0.806647</td>\n      <td>0.423011</td>\n      <td>287.852713</td>\n      <td>5.412109</td>\n      <td>0.607062</td>\n      <td>0.0</td>\n      <td>74.546994</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>65555044</th>\n      <td>373669</td>\n      <td>NaN</td>\n      <td>646.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>491.825028</td>\n      <td>7.181488</td>\n      <td>1.080613</td>\n      <td>13937.0</td>\n      <td>1648.350759</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>65555045</th>\n      <td>373670</td>\n      <td>NaN</td>\n      <td>647.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>492.201550</td>\n      <td>7.321686</td>\n      <td>1.082030</td>\n      <td>13937.0</td>\n      <td>1648.158269</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>65555046</th>\n      <td>373671</td>\n      <td>NaN</td>\n      <td>648.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>494.302326</td>\n      <td>8.170197</td>\n      <td>1.103524</td>\n      <td>13937.0</td>\n      <td>1649.082272</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>65555047</th>\n      <td>373672</td>\n      <td>NaN</td>\n      <td>649.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>495.217054</td>\n      <td>8.179382</td>\n      <td>1.150940</td>\n      <td>13937.0</td>\n      <td>1647.846105</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>65555048</th>\n      <td>373673</td>\n      <td>NaN</td>\n      <td>650.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>495.193798</td>\n      <td>8.164246</td>\n      <td>1.109885</td>\n      <td>13937.0</td>\n      <td>1648.669199</td>\n      <td>...</td>\n      <td>0.638298</td>\n      <td>0.053192</td>\n      <td>1.22893</td>\n      <td>0.828522</td>\n      <td>1.293989</td>\n      <td>12.0</td>\n      <td>1.643925</td>\n      <td>0.358465</td>\n      <td>-0.210992</td>\n      <td>1.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>65555049 rows × 29 columns</p>\n</div>",
            "text/plain": "          Unnamed: 0  Unnamed: 0.1  Frame     Gauss      MSDs  Mean_Intensity  \\\n0                  0           0.0    0.0  0.000000  0.000000      288.015504   \n1                  1           1.0    1.0  0.753676  0.371693      288.348837   \n2                  2           2.0    2.0  0.797005  0.385012      290.767442   \n3                  3           3.0    3.0  0.863716  0.401320      289.759690   \n4                  4           4.0    4.0  0.806647  0.423011      287.852713   \n...              ...           ...    ...       ...       ...             ...   \n65555044      373669           NaN  646.0       NaN       NaN      491.825028   \n65555045      373670           NaN  647.0       NaN       NaN      492.201550   \n65555046      373671           NaN  648.0       NaN       NaN      494.302326   \n65555047      373672           NaN  649.0       NaN       NaN      495.217054   \n65555048      373673           NaN  650.0       NaN       NaN      495.193798   \n\n           Quality  SN_Ratio  Track_ID            X  ...  asymmetry3  \\\n0         5.162109  0.701482       0.0    74.264553  ...         NaN   \n1         5.122284  0.643396       0.0    74.564264  ...         NaN   \n2         5.700928  0.706083       0.0    74.879671  ...         NaN   \n3         5.593384  0.756702       0.0    73.889012  ...         NaN   \n4         5.412109  0.607062       0.0    74.546994  ...         NaN   \n...            ...       ...       ...          ...  ...         ...   \n65555044  7.181488  1.080613   13937.0  1648.350759  ...         NaN   \n65555045  7.321686  1.082030   13937.0  1648.158269  ...         NaN   \n65555046  8.170197  1.103524   13937.0  1649.082272  ...         NaN   \n65555047  8.179382  1.150940   13937.0  1647.846105  ...         NaN   \n65555048  8.164246  1.109885   13937.0  1648.669199  ...    0.638298   \n\n          boundedness  efficiency  elongation  fractal_dim  frames  kurtosis  \\\n0                 NaN         NaN         NaN          NaN     NaN       NaN   \n1                 NaN         NaN         NaN          NaN     NaN       NaN   \n2                 NaN         NaN         NaN          NaN     NaN       NaN   \n3                 NaN         NaN         NaN          NaN     NaN       NaN   \n4                 NaN         NaN         NaN          NaN     NaN       NaN   \n...               ...         ...         ...          ...     ...       ...   \n65555044          NaN         NaN         NaN          NaN     NaN       NaN   \n65555045          NaN         NaN         NaN          NaN     NaN       NaN   \n65555046          NaN         NaN         NaN          NaN     NaN       NaN   \n65555047          NaN         NaN         NaN          NaN     NaN       NaN   \n65555048     0.053192     1.22893    0.828522     1.293989    12.0  1.643925   \n\n          straightness  trappedness  region  \n0                  NaN          NaN     NaN  \n1                  NaN          NaN     NaN  \n2                  NaN          NaN     NaN  \n3                  NaN          NaN     NaN  \n4                  NaN          NaN     NaN  \n...                ...          ...     ...  \n65555044           NaN          NaN     NaN  \n65555045           NaN          NaN     NaN  \n65555046           NaN          NaN     NaN  \n65555047           NaN          NaN     NaN  \n65555048      0.358465    -0.210992     1.0  \n\n[65555049 rows x 29 columns]"
          },
          "metadata": {}
        }
      ],
      "execution_count": 44,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "filename = 'features_NT_slice_2_striatum_vid_5.csv'\n",
        "fstats = pd.read_csv(dataset_path + '/' + filename, encoding = \"ISO-8859-1\", index_col='Unnamed: 0')\n",
        "tstats = find_pair(dataset_path + '/' + filename)\n",
        "print('{} size: {}'.format(filename, fstats.shape))\n",
        "if 'cortex' in filename:\n",
        "    fstats['region'] = pd.Series(fstats.shape[0]*[0], index=fstats.index)\n",
        "elif 'striatum' in filename: \n",
        "    fstats['region'] = pd.Series(fstats.shape[0]*[1], index=fstats.index)\n",
        "else:\n",
        "    print('Error, no target')\n",
        "fstats = combine_track(tstats, feature=np.append(feat, ['region']), featureDF=fstats)\n",
        "if fstats_tot is None:\n",
        "    fstats_tot = fstats\n",
        "else:\n",
        "    fstats_tot = fstats_tot.append(fstats, ignore_index=True)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "features_NT_slice_2_striatum_vid_5.csv size: (13938, 23)\n"
          ]
        }
      ],
      "execution_count": 42,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# fstats_tot.to_csv('cortex_striatum_featuresandtracks.csv')"
      ],
      "outputs": [],
      "execution_count": 45,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "fstats_tot = pd.read_csv('saved_datasets/cortex_striatum_featuresandtracks.csv')"
      ],
      "outputs": [],
      "execution_count": 9,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "feat = np.array(['AR', 'D_fit', 'Deff1', 'Deff2', 'MSD_ratio', 'alpha',\n",
        "       'asymmetry1', 'asymmetry2', 'asymmetry3', 'boundedness',\n",
        "       'efficiency', 'elongation', 'fractal_dim', 'frames', 'kurtosis',\n",
        "       'straightness', 'trappedness'])"
      ],
      "outputs": [],
      "execution_count": 9,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "def zero_df(df, col, res=(0, 651)):\n",
        "    '''\n",
        "    Zeros a single dataframe column so that the first value will be\n",
        "    located at the start of the track.\n",
        "    '''\n",
        "    try:\n",
        "        shift_val = df.iloc[res[0]:res[1]][col].reset_index().dropna().index[0]\n",
        "    except:\n",
        "        shift_val = res[0]-res[1]-1\n",
        "    return df.iloc[res[0]:res[1]][col].reset_index().shift(-shift_val, fill_value=np.nan)[col]\n",
        "\n",
        "def get_zeroed_tracks(df, col, res=650):\n",
        "    '''\n",
        "    Creates an array of all the tracks for a single column in a file\n",
        "    in which the value is zeroed to frame = 0\n",
        "    '''\n",
        "    lower = 0\n",
        "    upper = res+1\n",
        "    value = []\n",
        "    while (upper <= len(df)):\n",
        "        value.append(list(zero_df(df, col=col, res=[lower, upper])))\n",
        "        lower = upper\n",
        "        upper = lower + res + 1\n",
        "    return value"
      ],
      "outputs": [],
      "execution_count": 12,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Creates x and y datasets for LSTM based off of input\n",
        "# track_df data\n",
        "def get_xy_data(df, target, feat=None, use_feat=False, res=650):\n",
        "    n_tracks = int((len(df))/(res+1))\n",
        "    frame = get_zeroed_tracks(df, 'Frame', res=res)\n",
        "    X = get_zeroed_tracks(df, 'X', res=res)\n",
        "    Y = get_zeroed_tracks(df, 'Y', res=res)\n",
        "    MSDs = get_zeroed_tracks(df, 'MSDs', res=res)\n",
        "    trgt = df[target]\n",
        "    datax = []\n",
        "    datay = []\n",
        "    datafeat = []\n",
        "    print(n_tracks)\n",
        "    for j in range(n_tracks):\n",
        "        trackx = []\n",
        "        tracky = []\n",
        "        trackfeat = []\n",
        "        for i in range(res+1):\n",
        "            trackx.append([int(frame[j][i]), X[j][i], Y[j][i], MSDs[j][i]])\n",
        "        datax.append(trackx)\n",
        "        del(trackx)\n",
        "        tracky.append(trgt[(res+1)*(j+1)-1])\n",
        "        datay.append(tracky)\n",
        "        del(tracky)\n",
        "        if use_feat is True:\n",
        "            trackfeat.append(list(df.loc[(res+1)*(j+1)-1, feat]))\n",
        "        datafeat.append(trackfeat)\n",
        "        del(trackfeat)\n",
        "    del(df, frame, X, Y, MSDs, trgt)\n",
        "    datax = np.array(datax)\n",
        "    datax = datax.reshape(n_tracks, res+1, 4)\n",
        "    datay = np.array(datay)\n",
        "    datay = datay.reshape(n_tracks, 1)\n",
        "    datafeat = np.array(datafeat)\n",
        "    datafeat = datafeat.reshape(n_tracks, len(feat))\n",
        "    result = [datax, datay]\n",
        "    if use_feat is True:\n",
        "        result += [datafeat]\n",
        "    return tuple(result)"
      ],
      "outputs": [],
      "execution_count": 13,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "feat"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 60,
          "data": {
            "text/plain": "array(['AR', 'D_fit', 'Deff1', 'Deff2', 'MSD_ratio', 'alpha',\n       'asymmetry1', 'asymmetry2', 'asymmetry3', 'boundedness',\n       'efficiency', 'elongation', 'fractal_dim', 'frames', 'kurtosis',\n       'straightness', 'trappedness'], dtype='<U12')"
          },
          "metadata": {}
        }
      ],
      "execution_count": 60,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "(datax, datay, datafeat) = get_xy_data(fstats_tot, 'region', feat, True)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "86761\n"
          ]
        }
      ],
      "execution_count": 14,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "def get_track(df, track, res):\n",
        "    return df.loc[(res+1)*(track):(res+1)*(track+1)-1]\n",
        "\n",
        "def get_feat(df, track, res, feat):\n",
        "    return df.loc[(res+1)*(track+1)-1, feat]"
      ],
      "outputs": [],
      "execution_count": 16,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "np.save('./saved_datasets/RNN_region_datax', datax)\n",
        "np.save('./saved_datasets/RNN_region_datay', datay)\n",
        "np.save('./saved_datasets/RNN_region_datafeat', datafeat)\n",
        "# datax = np.load('./saved_datasets/RNN_region_datax.npy')\n",
        "# datay = np.load('./saved_datasets/RNN_region_datay.npy')\n",
        "# datafeat = np.load('./saved_datasets/RNN_region_datafeat.npy')"
      ],
      "outputs": [],
      "execution_count": 15,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "split = 0.8\n",
        "train_index = np.random.choice(np.arange(0, len(datax)), int(len(datax)*0.7), replace=False)\n",
        "test_index = np.setdiff1d(np.arange(0, len(datax)), train_index)\n",
        "datax = np.nan_to_num(datax, copy=True, nan=-1.0, posinf=-1.0, neginf=-1.0)\n",
        "datay = np.nan_to_num(datay, copy=True, nan=-1.0, posinf=-1.0, neginf=-1.0)\n",
        "X_train = datax[train_index]\n",
        "y_train = datay[train_index]\n",
        "feat_train = datafeat[train_index]\n",
        "X_test = datax[test_index]\n",
        "y_test = datay[test_index]\n",
        "feat_test = datafeat[test_index]"
      ],
      "outputs": [],
      "execution_count": 23,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "def numpy_one_hot_encode(mat, encoder=None):\n",
        "    if encoder is None:\n",
        "        encoder = np.unique(mat)\n",
        "    mat = np.array(encoder == mat).astype(int)\n",
        "    return mat, encoder\n",
        "y_train, encoder = numpy_one_hot_encode(y_train)\n",
        "y_test, encoder = numpy_one_hot_encode(y_test, encoder)"
      ],
      "outputs": [],
      "execution_count": 24,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "def numpy_decode(mat, encoder):\n",
        "    return np.array([i[i!=0] for i in mat * encoder])\n",
        "y_train = numpy_decode(y_train, encoder)\n",
        "y_test = numpy_decode(y_test, encoder)"
      ],
      "outputs": [],
      "execution_count": 19,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "n_timesteps, n_features, n_outputs = X_train.shape[1], X_train.shape[2], y_train.shape[1]\n",
        "(n_timesteps, n_features, n_outputs)"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 25,
          "data": {
            "text/plain": "(651, 4, 2)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 25,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "n_samples, n_feat_size = feat_train.shape\n",
        "(n_samples, n_feat_size)"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 26,
          "data": {
            "text/plain": "(60732, 17)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 26,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "#Kera libraries\n",
        "import numpy\n",
        "from tensorflow.keras.datasets import imdb\n",
        "from tensorflow.keras.models import Sequential, Model, load_model\n",
        "from tensorflow.keras.layers import Dense, LSTM, Input, Dropout, Concatenate, Flatten, TimeDistributed\n",
        "from tensorflow.keras.preprocessing import sequence"
      ],
      "outputs": [],
      "execution_count": 27,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# LSTM without dropout for sequence classification in the IMDB dataset\n",
        "import numpy\n",
        "from tensorflow.keras.datasets import imdb\n",
        "from tensorflow.keras.models import Sequential\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import LSTM\n",
        "from tensorflow.keras.layers import Embedding\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "\n",
        "def rnn_clsfy(X_train, y_train, n_timesteps, n_features, n_outputs, epochs=15, batch_size=64, verbose=0, **kwargs):\n",
        "    if 'dropout' not in kwargs:\n",
        "        dropout = 0.5\n",
        "    else:\n",
        "        dropout = kwargs['dropout']\n",
        "    if 'seed' not in kwargs:\n",
        "        seed = 123\n",
        "    else:\n",
        "        seed = kwargs['seed']\n",
        "    if 'metrics' not in kwargs:\n",
        "        metrics = ['accuracy']\n",
        "    else:\n",
        "        metrics = kwargs['metrics']\n",
        "    if 'n_rnnnodes' not in kwargs:\n",
        "        n_rnnnodes = 100\n",
        "    else:\n",
        "        n_rnnnodes = kwargs['n_rnnnodes']\n",
        "    # create the model\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(n_rnnnodes, input_shape=(n_timesteps, n_features), return_sequences=False))\n",
        "    model.add(Dropout(dropout))\n",
        "    model.add(Dense(100, activation='relu'))\n",
        "    model.add(Dense(n_outputs, activation='softmax'))\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=metrics)\n",
        "    print(model.summary())\n",
        "    model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size)\n",
        "    # Final evaluation:\n",
        "    score = model.evaluate(X_test, y_test, batch_size=batch_size, verbose=verbose)\n",
        "    print(f'Accuracy: {score[1]}')\n",
        "    return model\n",
        "\n",
        "model = rnn_clsfy(X_train, y_train, n_timesteps, n_features, n_outputs, epochs=50, batch_size=100, verbose=0, dropout=0.4, seed=10, metrics = ['accuracy'])"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm (LSTM)                  (None, 100)               42000     \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 2)                 202       \n",
            "=================================================================\n",
            "Total params: 52,302\n",
            "Trainable params: 52,302\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Train on 60732 samples\n",
            "Epoch 1/50\n",
            "60732/60732 [==============================] - 31s 507us/sample - loss: 0.6270 - accuracy: 0.5889\n",
            "Epoch 2/50\n",
            "60732/60732 [==============================] - 27s 441us/sample - loss: 0.6070 - accuracy: 0.6114\n",
            "Epoch 3/50\n",
            "60732/60732 [==============================] - 27s 453us/sample - loss: 0.6005 - accuracy: 0.6252\n",
            "Epoch 4/50\n",
            "60732/60732 [==============================] - 27s 450us/sample - loss: 0.5938 - accuracy: 0.6347\n",
            "Epoch 5/50\n",
            "60732/60732 [==============================] - 28s 463us/sample - loss: 0.5920 - accuracy: 0.6400- loss: 0.5921 - accuracy: 0.\n",
            "Epoch 6/50\n",
            "60732/60732 [==============================] - 27s 443us/sample - loss: 0.5873 - accuracy: 0.6452- loss:\n",
            "Epoch 7/50\n",
            "60732/60732 [==============================] - 27s 441us/sample - loss: 0.5832 - accuracy: 0.6493\n",
            "Epoch 8/50\n",
            "60732/60732 [==============================] - 27s 440us/sample - loss: 0.5794 - accuracy: 0.6544\n",
            "Epoch 9/50\n",
            "60732/60732 [==============================] - 27s 441us/sample - loss: 0.5760 - accuracy: 0.6583- loss: 0.5758 - accura - ETA: 1s -\n",
            "Epoch 10/50\n",
            "60732/60732 [==============================] - 27s 442us/sample - loss: 0.5724 - accuracy: 0.6631\n",
            "Epoch 11/50\n",
            "60732/60732 [==============================] - 27s 442us/sample - loss: 0.5722 - accuracy: 0.6597\n",
            "Epoch 12/50\n",
            "60732/60732 [==============================] - 27s 439us/sample - loss: 0.5677 - accuracy: 0.6699- loss: 0.5678 - accu\n",
            "Epoch 13/50\n",
            "60732/60732 [==============================] - 27s 445us/sample - loss: 0.5680 - accuracy: 0.6645\n",
            "Epoch 14/50\n",
            "60732/60732 [==============================] - 27s 445us/sample - loss: 0.5671 - accuracy: 0.6678\n",
            "Epoch 15/50\n",
            "60732/60732 [==============================] - 27s 447us/sample - loss: 0.5693 - accuracy: 0.6623\n",
            "Epoch 16/50\n",
            "60732/60732 [==============================] - 28s 457us/sample - loss: 0.5675 - accuracy: 0.6659\n",
            "Epoch 17/50\n",
            "60732/60732 [==============================] - 27s 440us/sample - loss: 0.5666 - accuracy: 0.6675\n",
            "Epoch 18/50\n",
            "60732/60732 [==============================] - 27s 440us/sample - loss: 0.5684 - accuracy: 0.6675\n",
            "Epoch 19/50\n",
            "60732/60732 [==============================] - 27s 440us/sample - loss: 0.5681 - accuracy: 0.6674\n",
            "Epoch 20/50\n",
            "60732/60732 [==============================] - 27s 445us/sample - loss: 0.5683 - accuracy: 0.6657\n",
            "Epoch 21/50\n",
            "60732/60732 [==============================] - 27s 442us/sample - loss: 0.5646 - accuracy: 0.6727\n",
            "Epoch 22/50\n",
            "60732/60732 [==============================] - 27s 440us/sample - loss: 0.5672 - accuracy: 0.6679\n",
            "Epoch 23/50\n",
            "60732/60732 [==============================] - 27s 440us/sample - loss: 0.5663 - accuracy: 0.6684- loss: 0.5665 - accura\n",
            "Epoch 24/50\n",
            "60732/60732 [==============================] - 27s 440us/sample - loss: 0.5630 - accuracy: 0.6738\n",
            "Epoch 25/50\n",
            "60732/60732 [==============================] - 27s 444us/sample - loss: 0.5657 - accuracy: 0.6674\n",
            "Epoch 26/50\n",
            "60732/60732 [==============================] - 27s 440us/sample - loss: 0.5669 - accuracy: 0.6659\n",
            "Epoch 27/50\n",
            "60732/60732 [==============================] - 27s 439us/sample - loss: 0.5614 - accuracy: 0.6761\n",
            "Epoch 28/50\n",
            "60732/60732 [==============================] - 27s 439us/sample - loss: 0.5603 - accuracy: 0.6744\n",
            "Epoch 29/50\n",
            "60732/60732 [==============================] - 27s 439us/sample - loss: 0.5621 - accuracy: 0.6726\n",
            "Epoch 30/50\n",
            "60732/60732 [==============================] - 27s 439us/sample - loss: 0.5620 - accuracy: 0.6750\n",
            "Epoch 31/50\n",
            "60732/60732 [==============================] - 27s 439us/sample - loss: 0.5733 - accuracy: 0.6496\n",
            "Epoch 32/50\n",
            "60732/60732 [==============================] - 27s 439us/sample - loss: 0.5673 - accuracy: 0.6608\n",
            "Epoch 33/50\n",
            "60732/60732 [==============================] - 27s 439us/sample - loss: 0.5619 - accuracy: 0.6712- loss: 0.5621 - ac\n",
            "Epoch 34/50\n",
            "60732/60732 [==============================] - 27s 439us/sample - loss: 0.5615 - accuracy: 0.6728- loss: 0.5614 - accura - ETA: 0s - loss: 0.5613 - \n",
            "Epoch 35/50\n",
            "60732/60732 [==============================] - 27s 439us/sample - loss: 0.5580 - accuracy: 0.6749\n",
            "Epoch 36/50\n",
            "60732/60732 [==============================] - 27s 440us/sample - loss: 0.5577 - accuracy: 0.6769\n",
            "Epoch 37/50\n",
            "60732/60732 [==============================] - 27s 441us/sample - loss: 0.5574 - accuracy: 0.6780\n",
            "Epoch 38/50\n",
            "60732/60732 [==============================] - 27s 439us/sample - loss: 0.5579 - accuracy: 0.6777-\n",
            "Epoch 39/50\n",
            "60732/60732 [==============================] - 27s 439us/sample - loss: 0.5568 - accuracy: 0.6779\n",
            "Epoch 40/50\n",
            "60732/60732 [==============================] - 27s 441us/sample - loss: 0.5572 - accuracy: 0.6762\n",
            "Epoch 41/50\n",
            "60732/60732 [==============================] - 27s 439us/sample - loss: 0.5565 - accuracy: 0.6785- loss: 0.5562 - accura\n",
            "Epoch 42/50\n",
            "60732/60732 [==============================] - 27s 439us/sample - loss: 0.5591 - accuracy: 0.6730\n",
            "Epoch 43/50\n",
            "60732/60732 [==============================] - 27s 442us/sample - loss: 0.5567 - accuracy: 0.6765\n",
            "Epoch 44/50\n",
            "60732/60732 [==============================] - 27s 450us/sample - loss: 0.5554 - accuracy: 0.6784\n",
            "Epoch 45/50\n",
            "60732/60732 [==============================] - 27s 451us/sample - loss: 0.5550 - accuracy: 0.6817\n",
            "Epoch 46/50\n",
            "60732/60732 [==============================] - 27s 443us/sample - loss: 0.5570 - accuracy: 0.6768\n",
            "Epoch 47/50\n",
            "60732/60732 [==============================] - 27s 442us/sample - loss: 0.5559 - accuracy: 0.6773\n",
            "Epoch 48/50\n",
            "60732/60732 [==============================] - 27s 443us/sample - loss: 0.5550 - accuracy: 0.6786\n",
            "Epoch 49/50\n",
            "60732/60732 [==============================] - 27s 441us/sample - loss: 0.5539 - accuracy: 0.6815\n",
            "Epoch 50/50\n",
            "60732/60732 [==============================] - 64s 1ms/sample - loss: 0.5551 - accuracy: 0.6806\n",
            "Accuracy: 0.6855046153068542\n"
          ]
        }
      ],
      "execution_count": 28,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('.\\saved_models\\LSTM_RNN_MODEL_50_50_SPLIT_Striatum_Cortex_TARGET_JUL3020_DATE_50_EPOCHS_40_DROPOUT_SHACK')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "model = load_model('.\\saved_models\\LSTM_RNN_MODEL_70_20_SPLIT_Striatum_Cortex_TARGET_JUL3020_DATE_SHACK')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from utils.constants import MAX_NB_VARIABLES, MAX_TIMESTEPS_LIST\n",
        "from utils.generic_utils import load_dataset_at, calculate_dataset_metrics, cutoff_choice, \\\n",
        "    cutoff_sequence\n",
        "from keras import backend as K\n",
        "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "from keras.optimizers import Adam\n",
        "from keras.layers import Permute\n",
        "from keras.models import Model\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "import warnings\n",
        "warnings.simplefilter('ignore', category=DeprecationWarning)\n",
        "\n",
        "\n",
        "def multi_label_log_loss(y_pred, y_true):\n",
        "    return K.sum(K.binary_crossentropy(y_pred, y_true), axis=-1)\n",
        "\n",
        "\n",
        "def _average_gradient_norm(model, X_train, y_train, batch_size):\n",
        "    # just checking if the model was already compiled\n",
        "    if not hasattr(model, \"train_function\"):\n",
        "        raise RuntimeError(\"You must compile your model before using it.\")\n",
        "\n",
        "    weights = model.trainable_weights  # weight tensors\n",
        "\n",
        "    get_gradients = model.optimizer.get_gradients(\n",
        "        model.total_loss, weights)  # gradient tensors\n",
        "\n",
        "    input_tensors = [\n",
        "        # input data\n",
        "        model.inputs[0],\n",
        "        # how much to weight each sample by\n",
        "        model.sample_weights[0],\n",
        "        # labels\n",
        "        model.targets[0],\n",
        "        # train or test mode\n",
        "        K.learning_phase()\n",
        "    ]\n",
        "\n",
        "    grad_fct = K.function(inputs=input_tensors, outputs=get_gradients)\n",
        "\n",
        "    steps = 0\n",
        "    total_norm = 0\n",
        "    s_w = None\n",
        "\n",
        "    nb_steps = X_train.shape[0] // batch_size\n",
        "\n",
        "    if X_train.shape[0] % batch_size == 0:\n",
        "        pad_last = False\n",
        "    else:\n",
        "        pad_last = True\n",
        "\n",
        "    def generator(X_train, y_train, pad_last):\n",
        "        for i in range(nb_steps):\n",
        "            X = X_train[i * batch_size: (i + 1) * batch_size, ...]\n",
        "            y = y_train[i * batch_size: (i + 1) * batch_size, ...]\n",
        "\n",
        "            yield (X, y)\n",
        "\n",
        "        if pad_last:\n",
        "            X = X_train[nb_steps * batch_size:, ...]\n",
        "            y = y_train[nb_steps * batch_size:, ...]\n",
        "\n",
        "            yield (X, y)\n",
        "\n",
        "    datagen = generator(X_train, y_train, pad_last)\n",
        "\n",
        "    while steps < nb_steps:\n",
        "        X, y = next(datagen)\n",
        "        # set sample weights to one\n",
        "        # for every input\n",
        "        if s_w is None:\n",
        "            s_w = np.ones(X.shape[0])\n",
        "\n",
        "        gradients = grad_fct([X, s_w, y, 0])\n",
        "        total_norm += np.sqrt(np.sum([np.sum(np.square(g))\n",
        "                                      for g in gradients]))\n",
        "        steps += 1\n",
        "\n",
        "    if pad_last:\n",
        "        X, y = next(datagen)\n",
        "        # set sample weights to one\n",
        "        # for every input\n",
        "        if s_w is None:\n",
        "            s_w = np.ones(X.shape[0])\n",
        "\n",
        "        gradients = grad_fct([X, s_w, y, 0])\n",
        "        total_norm += np.sqrt(np.sum([np.sum(np.square(g))\n",
        "                                      for g in gradients]))\n",
        "        steps += 1\n",
        "\n",
        "    return total_norm / float(steps)\n",
        "\n",
        "\n",
        "def rnn_train_model(model: Model, \n",
        "                    train_dataset, \n",
        "                    eval_dataset,\n",
        "                    folds=5, \n",
        "                    epochs=50, \n",
        "                    batch_size=128, \n",
        "                    val_subset=None,\n",
        "                    cutoff=None,  \n",
        "                    learning_rate=1e-3, \n",
        "                    monitor='loss', \n",
        "                    optimization_mode='auto', \n",
        "                    compile_model=True):\n",
        "    \n",
        "    X_train, y_train, X_test, y_test, is_timeseries = load_dataset_at(dataset_id,\n",
        "                                                                      fold_index=dataset_fold_id,\n",
        "                                                                      normalize_timeseries=normalize_timeseries)\n",
        "    max_timesteps, max_nb_variables = calculate_dataset_metrics(X_train)\n",
        "\n",
        "    if max_nb_variables != MAX_NB_VARIABLES[dataset_id]:\n",
        "        if cutoff is None:\n",
        "            choice = cutoff_choice(dataset_id, max_nb_variables)\n",
        "        else:\n",
        "            assert cutoff in [\n",
        "                'pre', 'post'], 'Cutoff parameter value must be either \"pre\" or \"post\"'\n",
        "            choice = cutoff\n",
        "\n",
        "        if choice not in ['pre', 'post']:\n",
        "            return\n",
        "        else:\n",
        "            X_train, X_test = cutoff_sequence(\n",
        "                X_train, X_test, choice, dataset_id, max_nb_variables)\n",
        "            \n",
        "    classes = np.unique(y_train)\n",
        "    le = LabelEncoder()\n",
        "    y_ind = le.fit_transform(y_train.ravel())\n",
        "    recip_freq = len(y_train) / (len(le.classes_) *\n",
        "                                 np.bincount(y_ind).astype(np.float64))\n",
        "    class_weight = recip_freq[le.transform(classes)]\n",
        "\n",
        "    print(\"Class weights : \", class_weight)\n",
        "\n",
        "    y_train = to_categorical(y_train, len(np.unique(y_train)))\n",
        "    y_test = to_categorical(y_test, len(np.unique(y_test)))\n",
        "\n",
        "    if is_timeseries:\n",
        "        factor = 1./np.cbrt(2)\n",
        "    else:\n",
        "        factor = 1./np.sqrt(2)\n",
        "\n",
        "    if dataset_fold_id is None:\n",
        "        weight_fn = \"./weights/%s_weights.h5\" % dataset_prefix\n",
        "    else:\n",
        "        weight_fn = \"./weights/%s_fold_%d_weights.h5\" % (\n",
        "            dataset_prefix, dataset_fold_id)\n",
        "\n",
        "    model_checkpoint = ModelCheckpoint(weight_fn, verbose=1, mode=optimization_mode,\n",
        "                                       monitor=monitor, save_best_only=True, save_weights_only=True)\n",
        "    reduce_lr = ReduceLROnPlateau(monitor=monitor, patience=100, mode=optimization_mode,\n",
        "                                  factor=factor, cooldown=0, min_lr=1e-4, verbose=2)\n",
        "    callback_list = [model_checkpoint, reduce_lr]\n",
        "\n",
        "    optm = Adam(lr=learning_rate)\n",
        "\n",
        "    if compile_model:\n",
        "        model.compile(optimizer=optm,\n",
        "                      loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    if val_subset is not None:\n",
        "        X_test = X_test[:val_subset]\n",
        "        y_test = y_test[:val_subset]\n",
        "\n",
        "    model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, callbacks=callback_list,\n",
        "              class_weight=class_weight, verbose=2, validation_data=(X_test, y_test))\n",
        "\n",
        "\n",
        "def evaluate_model(model: Model, dataset_id, dataset_prefix, dataset_fold_id=None, batch_size=128, test_data_subset=None,\n",
        "                   cutoff=None, normalize_timeseries=False):\n",
        "    _, _, X_test, y_test, is_timeseries = load_dataset_at(dataset_id,\n",
        "                                                          fold_index=dataset_fold_id,\n",
        "                                                          normalize_timeseries=normalize_timeseries)\n",
        "    max_timesteps, max_nb_variables = calculate_dataset_metrics(X_test)\n",
        "\n",
        "    if max_nb_variables != MAX_NB_VARIABLES[dataset_id]:\n",
        "        if cutoff is None:\n",
        "            choice = cutoff_choice(dataset_id, max_nb_variables)\n",
        "        else:\n",
        "            assert cutoff in [\n",
        "                'pre', 'post'], 'Cutoff parameter value must be either \"pre\" or \"post\"'\n",
        "            choice = cutoff\n",
        "\n",
        "        if choice not in ['pre', 'post']:\n",
        "            return\n",
        "        else:\n",
        "            _, X_test = cutoff_sequence(\n",
        "                None, X_test, choice, dataset_id, max_nb_variables)\n",
        "\n",
        "    if not is_timeseries:\n",
        "        X_test = pad_sequences(\n",
        "            X_test, maxlen=MAX_NB_VARIABLES[dataset_id], padding='post', truncating='post')\n",
        "    y_test = to_categorical(y_test, len(np.unique(y_test)))\n",
        "\n",
        "    optm = Adam(lr=1e-3)\n",
        "    model.compile(optimizer=optm, loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    if dataset_fold_id is None:\n",
        "        weight_fn = \"./weights/%s_weights.h5\" % dataset_prefix\n",
        "    else:\n",
        "        weight_fn = \"./weights/%s_fold_%d_weights.h5\" % (\n",
        "            dataset_prefix, dataset_fold_id)\n",
        "    model.load_weights(weight_fn)\n",
        "\n",
        "    if test_data_subset is not None:\n",
        "        X_test = X_test[:test_data_subset]\n",
        "        y_test = y_test[:test_data_subset]\n",
        "\n",
        "    print(\"\\nEvaluating : \")\n",
        "    loss, accuracy = model.evaluate(X_test, y_test, batch_size=batch_size)\n",
        "    print()\n",
        "    print(\"Final Accuracy : \", accuracy)\n",
        "\n",
        "    return accuracy, loss\n",
        "\n",
        "\n",
        "def set_trainable(layer, value):\n",
        "    layer.trainable = value\n",
        "\n",
        "    # case: container\n",
        "    if hasattr(layer, 'layers'):\n",
        "        for l in layer.layers:\n",
        "            set_trainable(l, value)\n",
        "\n",
        "    # case: wrapper (which is a case not covered by the PR)\n",
        "    if hasattr(layer, 'layer'):\n",
        "        set_trainable(layer.layer, value)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy\n",
        "from keras.datasets import imdb\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.preprocessing import sequence\n",
        "# fix random seed for reproducibility\n",
        "numpy.random.seed(7)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using TensorFlow backend.\n"
          ]
        }
      ],
      "execution_count": 1,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# load the dataset but only keep the top n words, zero the rest\n",
        "top_words = 5000"
      ],
      "outputs": [],
      "execution_count": 2,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "# save np.load\n",
        "#np_load_old = np.load\n",
        "\n",
        "# modify the default parameters of np.load\n",
        "#np.load = lambda *a,**k: np_load_old(*a, allow_pickle=True, **k)\n",
        "\n",
        "# call load_data with allow_pickle implicitly set to true\n",
        "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)\n",
        "\n",
        "# restore np.load for future normal usage\n",
        "#np.load = np_load_old"
      ],
      "outputs": [],
      "execution_count": 3,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# truncate and pad input sequences\n",
        "max_review_length = 500\n",
        "X_train = sequence.pad_sequences(X_train, maxlen=max_review_length)\n",
        "X_test = sequence.pad_sequences(X_test, maxlen=max_review_length)"
      ],
      "outputs": [],
      "execution_count": 4,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy\n",
        "from tensorflow.keras.datasets import imdb\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import LSTM\n",
        "from tensorflow.keras.layers import Embedding\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "# fix random seed for reproducibility\n",
        "numpy.random.seed(7)\n",
        "# create the model\n",
        "embedding_vecor_length = 32\n",
        "model = tf.keras.Sequential()\n",
        "# model = Sequential()\n",
        "model.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length))\n",
        "model.add(LSTM(100))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "print(model.summary())\n",
        "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=3, batch_size=64)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 500, 32)           160000    \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 100)               53200     \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 101       \n",
            "=================================================================\n",
            "Total params: 213,301\n",
            "Trainable params: 213,301\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Train on 25000 samples, validate on 25000 samples\n",
            "Epoch 1/3\n",
            "25000/25000 [==============================] - 20s 784us/sample - loss: 0.4684 - accuracy: 0.7700 - val_loss: 0.3720 - val_accuracy: 0.8354\n",
            "Epoch 2/3\n",
            "25000/25000 [==============================] - 18s 736us/sample - loss: 0.3134 - accuracy: 0.8718 - val_loss: 0.3179 - val_accuracy: 0.8743\n",
            "Epoch 3/3\n",
            "25000/25000 [==============================] - 18s 734us/sample - loss: 0.2483 - accuracy: 0.9035 - val_loss: 0.3082 - val_accuracy: 0.8743\n"
          ]
        },
        {
          "output_type": "execute_result",
          "execution_count": 21,
          "data": {
            "text/plain": "<tensorflow.python.keras.callbacks.History at 0x16ebc9168c8>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 21,
      "metadata": {
        "scrolled": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Final evaluation of the model\n",
        "scores = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 87.43%\n"
          ]
        }
      ],
      "execution_count": 22,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# LSTM with Dropout for sequence classification in the IMDB dataset\n",
        "import numpy\n",
        "from tensorflow.keras.datasets import imdb\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import LSTM\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import Embedding\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "\n",
        "# fix random seed for reproducibility\n",
        "numpy.random.seed(7)\n",
        "# load the dataset but only keep the top n words, zero the rest\n",
        "top_words = 5000\n",
        "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)\n",
        "# truncate and pad input sequences\n",
        "max_review_length = 500\n",
        "X_train = sequence.pad_sequences(X_train, maxlen=max_review_length)\n",
        "X_test = sequence.pad_sequences(X_test, maxlen=max_review_length)\n",
        "# create the model\n",
        "embedding_vecor_length = 32\n",
        "model = tf.keras.Sequential()\n",
        "model.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(100))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "print(model.summary())\n",
        "model.fit(X_train, y_train, epochs=3, batch_size=64)\n",
        "# Final evaluation of the model\n",
        "scores = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_3 (Embedding)      (None, 500, 32)           160000    \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 500, 32)           0         \n",
            "_________________________________________________________________\n",
            "lstm_3 (LSTM)                (None, 100)               53200     \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 1)                 101       \n",
            "=================================================================\n",
            "Total params: 213,301\n",
            "Trainable params: 213,301\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Train on 25000 samples\n",
            "Epoch 1/3\n",
            "25000/25000 [==============================] - 32s 1ms/sample - loss: 0.4913 - accuracy: 0.7535\n",
            "Epoch 2/3\n",
            "25000/25000 [==============================] - 27s 1ms/sample - loss: 0.3036 - accuracy: 0.8784\n",
            "Epoch 3/3\n",
            "25000/25000 [==============================] - 26s 1ms/sample - loss: 0.2707 - accuracy: 0.8929\n",
            "Accuracy: 87.10%\n"
          ]
        }
      ],
      "execution_count": 34,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# LSTM without dropout for sequence classification in the IMDB dataset\n",
        "import numpy\n",
        "from tensorflow.keras.datasets import imdb\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import LSTM\n",
        "from tensorflow.keras.layers import Embedding\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "\n",
        "# fix random seed for reproducibility\n",
        "numpy.random.seed(7)\n",
        "# load the dataset but only keep the top n words, zero the rest\n",
        "top_words = 5000\n",
        "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)\n",
        "# truncate and pad input sequences\n",
        "max_review_length = 500\n",
        "X_train = sequence.pad_sequences(X_train, maxlen=max_review_length)\n",
        "X_test = sequence.pad_sequences(X_test, maxlen=max_review_length)\n",
        "# create the model\n",
        "embedding_vecor_length = 32\n",
        "model = tf.keras.Sequential()\n",
        "model.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length))\n",
        "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "print(model.summary())\n",
        "model.fit(X_train, y_train, epochs=3, batch_size=64)\n",
        "# Final evaluation of the model\n",
        "scores = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_9\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_5 (Embedding)      (None, 500, 32)           160000    \n",
            "_________________________________________________________________\n",
            "lstm_5 (LSTM)                (None, 100)               53200     \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 1)                 101       \n",
            "=================================================================\n",
            "Total params: 213,301\n",
            "Trainable params: 213,301\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Train on 25000 samples\n",
            "Epoch 1/3\n",
            "25000/25000 [==============================] - 371s 15ms/sample - loss: 0.4980 - accuracy: 0.7517\n",
            "Epoch 2/3\n",
            "25000/25000 [==============================] - 310s 12ms/sample - loss: 0.3764 - accuracy: 0.8382\n",
            "Epoch 3/3\n",
            "25000/25000 [==============================] - 369s 15ms/sample - loss: 0.3311 - accuracy: 0.8636\n",
            "Accuracy: 85.66%\n"
          ]
        }
      ],
      "execution_count": 36,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# LSTM and CNN for sequence classification in the IMDB dataset\n",
        "import numpy\n",
        "from tensorflow.keras.datasets import imdb\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import LSTM\n",
        "from tensorflow.keras.layers.convolutional import Conv1D\n",
        "from tensorflow.keras.layers.convolutional import MaxPooling1D\n",
        "from tensorflow.keras.layers.embeddings import Embedding\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "# fix random seed for reproducibility\n",
        "numpy.random.seed(7)\n",
        "# load the dataset but only keep the top n words, zero the rest\n",
        "top_words = 5000\n",
        "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)\n",
        "# truncate and pad input sequences\n",
        "max_review_length = 500\n",
        "X_train = sequence.pad_sequences(X_train, maxlen=max_review_length)\n",
        "X_test = sequence.pad_sequences(X_test, maxlen=max_review_length)\n",
        "# create the model\n",
        "embedding_vecor_length = 32\n",
        "model = Sequential()\n",
        "model.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length))\n",
        "model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
        "model.add(MaxPooling1D(pool_size=2))\n",
        "model.add(LSTM(100))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "print(model.summary())\n",
        "model.fit(X_train, y_train, epochs=3, batch_size=64)\n",
        "# Final evaluation of the model\n",
        "scores = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
      ],
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'tensorflow.keras.layers.convolutional'",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-5-946ba871defd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDense\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLSTM\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvolutional\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mConv1D\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvolutional\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mMaxPooling1D\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membeddings\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mEmbedding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow.keras.layers.convolutional'"
          ]
        }
      ],
      "execution_count": 5,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "model.add(Embedding(n_most_common_words, emb_dim, input_length=X.shape[1]))\n",
        "model.add(SpatialDropout1D(0.7))\n",
        "model.add(LSTM(64, dropout=0.7, recurrent_dropout=0.7))\n",
        "model.add(Dense(4, activation='softmax'))\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "X_test"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 17,
          "data": {
            "text/plain": "array([[   0,    0,    0, ...,   14,    6,  717],\n       [   0,    0,    0, ...,  125,    4, 3077],\n       [  33,    6,   58, ...,    9,   57,  975],\n       ...,\n       [   0,    0,    0, ...,   21,  846,    2],\n       [   0,    0,    0, ..., 2302,    7,  470],\n       [   0,    0,    0, ...,   34, 2005, 2643]], dtype=int32)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 17,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# LSTM with Dropout for sequence classification in msd dataset\n",
        "import numpy\n",
        "from tensorflow.keras.datasets import imdb\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import LSTM\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from keras.layers.embeddings import Embedding\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "# fix random seed for reproducibility\n",
        "numpy.random.seed(7)\n",
        "# load the dataset b\n",
        "\n",
        "(X_train, y_train)\n",
        "(X_test, y_test)\n",
        "# truncate and pad input sequences\n",
        "max_review_length = 500\n",
        "X_train = sequence.pad_sequences(X_train, maxlen=max_review_length)\n",
        "X_test = sequence.pad_sequences(X_test, maxlen=max_review_length)\n",
        "# create the model\n",
        "embedding_vecor_length = 32\n",
        "model = Sequential()\n",
        "model.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(100))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "print(model.summary())\n",
        "model.fit(X_train, y_train, epochs=3, batch_size=64)\n",
        "# Final evaluation of the model\n",
        "scores = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
      ],
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'X_train' is not defined",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-7-11fe80a1b124>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;31m# load the dataset b\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;31m# truncate and pad input sequences\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mNameError\u001b[0m: name 'X_train' is not defined"
          ]
        }
      ],
      "execution_count": 7,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import collections\n",
        "from tensorflow.python.ops.math_ops import tanh\n",
        "\n",
        "\n",
        "class RNNCell(object):\n",
        "    def __call__(self, inputs, state, scope=None):\n",
        "        raise NotImplementedError(\"Abstract method\")\n",
        "    \n",
        "\n",
        "class LSTMCell(RNNCell):\n",
        "    \"\"\"Basic LSTM recurrent network cell.\n",
        "    The implementation is based on: http://arxiv.org/abs/1409.2329.\n",
        "    We add forget_bias (default: 1) to the biases of the forget gate in order to\n",
        "    reduce the scale of forgetting in the beginning of the training.\n",
        "    It does not allow cell clipping, a projection layer, and does not\n",
        "    use peep-hole connections: it is the basic baseline.\n",
        "    For advanced models, please use the full LSTMCell that follows.\n",
        "    \"\"\"  \n",
        "    def __init__(self, n_units, n_proj=None, forget_bias=0.0, input_size=None, activation=tanh):\n",
        "        self._n_units  = n_units\n",
        "        self._n_proj = n_proj\n",
        "        self._forget_bias = forget_bias\n",
        "        self._input_size = input_size\n",
        "        self._activation = activation\n",
        "\n",
        "        (self._state_size, \n",
        "         self._output_size) = ((LSTMStateTuple(n_units, n_proj) , n_units + n_proj)\n",
        "                            if n_proj else (LSTMStateTuple(n_units, n_units), 2*n_units))\n",
        "\n",
        "        \n",
        "    @property\n",
        "    def state_size(self):\n",
        "        return self._state_size\n",
        "    \n",
        "    \n",
        "    @property\n",
        "    def output_size(self):\n",
        "        return self. _output_size\n",
        "    \n",
        "    \n",
        "    def __call__(self, inputs, state, scope=None):\n",
        "        \n",
        "        pass\n",
        "\n",
        "# class LSTM(LSTM):\n",
        "    \n",
        "    \n",
        "#     def __init__(self, ):\n",
        "#         pass\n",
        "    \n",
        "_LSTMStateTuple = collections.namedtuple(\"LSTMStateTuple\", (\"c\", \"h\"))\n",
        "\n",
        "class LSTMStateTuple(_LSTMStateTuple):\n",
        "  \n",
        "    \"\"\"Tuple used by LSTM Cells for `state_size`, `zero_state`, and output state.\n",
        "    Stores two elements: `(c, h)`, in that order.\n",
        "    Only used when `state_is_tuple=True`.\n",
        "    \"\"\"\n",
        "    __slots__ = ()\n",
        "\n",
        "    @property\n",
        "    def dtype(self):\n",
        "        (c, h) = self\n",
        "        if not c.dtype == h.dtype:\n",
        "            raise TypeError(\"Inconsistent internal state: %s vs %s\" %\n",
        "                            (str(c.dtype), str(h.dtype)))\n",
        "    return c.dtype\n"
      ],
      "outputs": [],
      "execution_count": 12,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "x = LSTMCell(50, 20, 1.0, 128)\n"
      ],
      "outputs": [],
      "execution_count": 18,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "x.state_size"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 19,
          "data": {
            "text/plain": "LSTMStateTuple(c=50, h=20)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 19,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "x = LSTMCell()"
      ],
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "__init__() missing 1 required positional argument: 'n_units'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-ccebf5a8883c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLSTMCell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: __init__() missing 1 required positional argument: 'n_units'"
          ]
        }
      ],
      "execution_count": 20,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.9",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "toc": {
      "toc_position": {},
      "skip_h1_title": false,
      "number_sections": true,
      "title_cell": "Table of Contents",
      "toc_window_display": false,
      "base_numbering": 1,
      "toc_section_display": true,
      "title_sidebar": "Contents",
      "toc_cell": false,
      "nav_menu": {},
      "sideBar": true
    },
    "kernel_info": {
      "name": "python3"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}