{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from diff_predictor import data_process #comment this out if it causes issue - function is called below\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "from os import listdir, getcwd, chdir\n",
    "from os.path import isfile, join\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance_data(df, target, **kwargs):\n",
    "    \"\"\"\n",
    "    Balance spatial data using undersampling. Assumes input will\n",
    "    be a dataframe and data will be used for categorical classification\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        pandas dataframe to be balanced\n",
    "    target : string\n",
    "        the name of the target/tag/y-value column to balance data around\n",
    "        \n",
    "    Optional Parameters\n",
    "    -------------------\n",
    "    random_state : int : 1\n",
    "        seed to base random sampling from\n",
    "    Returns\n",
    "    -------\n",
    "    A fully balanced pandas dataframe\n",
    "    \"\"\"\n",
    "    if 'random_state' not in kwargs:\n",
    "        random_state = 1\n",
    "    else:\n",
    "        random_state = kwargs['random_state']\n",
    "    df_target = []\n",
    "    bal_df = []\n",
    "    for name in df[target].unique():\n",
    "        df_target.append((name, df[df[target] == name]))\n",
    "    print(f\"Ratio before data balance \" +\n",
    "          f\"({':'.join([str(i[0]) for i in df_target])}) = \" +\n",
    "          f\"{':'.join([str(len(i[1])) for i in df_target])}\")\n",
    "    for i in range(len(df_target)):\n",
    "        ratio = min([len(i[1]) for i in df_target])/len(df_target[i][1])\n",
    "        bal_df.append(df_target[i][1].sample(frac=ratio,\n",
    "                                             random_state=random_state))\n",
    "    print(f\"Ratio after balance \" +\n",
    "          f\"({':'.join([str(i[0]) for i in df_target])}) = \" +\n",
    "          f\"{':'.join([str(len(i)) for i in bal_df])}\")\n",
    "    assert len(bal_df) > 0, 'DataFrame cant be empty'\n",
    "    return pd.concat(bal_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_fullstats(dataset_path, filelist, targets, target_col_name='Target'):\n",
    "    \"\"\"\n",
    "    Generates single csv of all statatistics from list of files\n",
    "    Parameters\n",
    "    ---------\n",
    "    dataset_path: string\n",
    "        string of path to folder containing data files\n",
    "    filelist: list\n",
    "        list containing filenames of all files to be processed\n",
    "    targets: list\n",
    "        list containing strings that state which class/group a file is from,\n",
    "        string must be in the filename of the data files\n",
    "    Target: string\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    fstats_tot: pandas.DataFrame\n",
    "        dataframe containing all rows from data files and with new column\n",
    "        for the class/group the row came from\n",
    "    \"\"\"\n",
    "    fstats_tot = None\n",
    "    video_num = 0\n",
    "    for filename in filelist:\n",
    "            fstats = pd.read_csv(dataset_path + filename, encoding = \"ISO-8859-1\", index_col='Unnamed: 0')\n",
    "            #print('{} size: {}'.format(filename, fstats.shape))\n",
    "            \n",
    "            for i in range(0, len(targets)):\n",
    "                if targets[i] in filename:\n",
    "                    print('Adding file {} size: {}'.format(filename, fstats.shape))\n",
    "                    fstats[target_col_name] = pd.Series(fstats.shape[0]*[targets[i]], index=fstats.index)\n",
    "                    fstats['Filename'] = pd.Series(fstats.shape[0]*[filename], index=fstats.index)\n",
    "                    fstats['Video Number'] = pd.Series(fstats.shape[0]*[video_num], index=fstats.index)\n",
    "                    if fstats_tot is None:\n",
    "                        fstats_tot = fstats\n",
    "                    else:\n",
    "                        fstats_tot = fstats_tot.append(fstats, ignore_index=True)\n",
    "                    video_num += 1\n",
    "                    #break\n",
    "\n",
    "            \n",
    "    return fstats_tot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workbookDir = getcwd()\n",
    "\n",
    "print('Current Notebook Dir: ' + workbookDir)\n",
    "chdir(workbookDir) # Go to current workbook Dir\"\n",
    "chdir('..')        # Go up one\n",
    "print(f'Using current directory for loading data: {getcwd()}')\n",
    "workbookDir = getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You will have to change the data_path to what your own path to the data is to get it to run for you\n",
    "data_path = workbookDir + '/data/Brendan_traj_data/'\n",
    "filelist = [f for f in listdir(data_path) if isfile(join(data_path, f)) and 'feat' in f]\n",
    "print(len(filelist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the list of ['NT', '1uM', '5uM'] tells the function what classes/groups you want to split the data up by\n",
    "# In this case its split by treatment, so to split by time you could have:\n",
    "#   generate_fullstats(data_path, filelist, targets=['4DIV', '7DIV', '10DIV', target_col_name = 'time])\n",
    "fstats_tot = generate_fullstats(data_path, filelist, targets=['NT', '1uM', '5uM'], target_col_name='treatment')\n",
    "target = 'treatment' #for time analysis, would want to change this to whatever target_col_name is set to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_list = [\n",
    "    'alpha', # Fitted anomalous diffusion alpha exponenet\n",
    "    'D_fit', # Fitted anomalous diffusion coefficient\n",
    "    'kurtosis', # Kurtosis of track\n",
    "    'asymmetry1', # Asymmetry of trajecory (0 for circular symmetric, 1 for linear)\n",
    "    'asymmetry2', # Ratio of the smaller to larger principal radius of gyration\n",
    "    'asymmetry3', # An asymmetric feature that accnts for non-cylindrically symmetric pt distributions\n",
    "    'AR', # Aspect ratio of long and short side of trajectory's minimum bounding rectangle\n",
    "    'elongation', # Est. of amount of extension of trajectory from centroid\n",
    "    'boundedness', # How much a particle with Deff is restricted by a circular confinement of radius r\n",
    "    'fractal_dim', # Measure of how complicated a self similar figure is\n",
    "    'trappedness', # Probability that a particle with Deff is trapped in a region\n",
    "    'efficiency', # Ratio of squared net displacement to the sum of squared step lengths\n",
    "    'straightness', # Ratio of net displacement to the sum of squared step lengths\n",
    "    'MSD_ratio', # MSD ratio of the track\n",
    "    'Deff1', # Effective diffusion coefficient at 0.33 s\n",
    "    'Deff2', # Effective diffusion coefficient at 3.3 s\n",
    "    'Mean alpha', \n",
    "    'Mean D_fit', \n",
    "    'Mean kurtosis', \n",
    "    'Mean asymmetry1', \n",
    "    'Mean asymmetry2',\n",
    "    'Mean asymmetry3', \n",
    "    'Mean AR',\n",
    "    'Mean elongation', \n",
    "    'Mean boundedness',\n",
    "    'Mean fractal_dim', \n",
    "    'Mean trappedness', \n",
    "    'Mean efficiency',\n",
    "    'Mean straightness', \n",
    "    'Mean MSD_ratio', \n",
    "    'Mean Deff1', \n",
    "    'Mean Deff2',\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecm = fstats_tot[feature_list + [target, 'Track_ID', 'X', 'Y']] #dont think i need these rn\n",
    "print(ecm.shape)\n",
    "ecm = ecm[~ecm[list(set(feature_list) - set(['Deff2', 'Mean Deff2']))].isin([np.nan, np.inf, -np.inf]).any(1)]       # Removing nan and inf data points\n",
    "ecm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bal_ecm = balance_data(ecm, target)\n",
    "labels = ['NT', '1uM', '5uM']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=3, sharey=True)\n",
    "\n",
    "for i, group in enumerate(labels):\n",
    "    df = bal_ecm[bal_ecm[target]==labels[i]]\n",
    "    fracdim_arr = df['fractal_dim'].values\n",
    "    ax[i].hist(fracdim_arr, histtype='bar', bins=100) # histtype and bins parameters can \n",
    "                                                      #be played with: https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.hist.html\n",
    "    ax[i].set_xlabel(group)\n",
    "plt.suptitle('Distributions of Fractal Dimension')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=3, sharey=True)\n",
    "\n",
    "for i, group in enumerate(labels):\n",
    "    df = bal_ecm[bal_ecm[target]==labels[i]]\n",
    "    alpha_arr = df['alpha'].values\n",
    "    ax[i].hist(alpha_arr, histtype='bar', bins=100) # histtype and bins parameters can \n",
    "                                                      #be played with: https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.hist.html\n",
    "    ax[i].set_xlabel(group)\n",
    "plt.suptitle('Distributions of Alpha')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=3, sharey=True)\n",
    "\n",
    "for i, group in enumerate(labels):\n",
    "    df = bal_ecm[bal_ecm[target]==labels[i]]\n",
    "    alpha_arr = df['alpha'].values\n",
    "    ax[i].hist(np.log(alpha_arr), histtype='bar', bins=100) # histtype and bins parameters can \n",
    "                                                      #be played with: https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.hist.html\n",
    "    ax[i].set_xlabel(group)\n",
    "plt.suptitle('Distributions of log(Alpha)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(4,8))\n",
    "\n",
    "directed_percent = np.zeros(len(labels))\n",
    "normal_percent = np.zeros(len(labels))\n",
    "constrained_percent = np.zeros(len(labels))\n",
    "immobilized_percent = np.zeros(len(labels))\n",
    "\n",
    "for i, unique_class in enumerate(labels):\n",
    "    \n",
    "    df = bal_ecm[bal_ecm[target] == unique_class]\n",
    "\n",
    "    directed_df = df[df['alpha'] > 1.1]\n",
    "    directed_percent[i] = (len(directed_df)/len(df))\n",
    "\n",
    "    normal_df = df[(df['alpha'] <= 1.1) & (df['alpha'] >= 0.9)]\n",
    "    normal_percent[i] = (len(normal_df)/len(df))\n",
    "    \n",
    "    constrained_df = df[(df['alpha'] < 0.9)]\n",
    "    constrained_percent[i] = (len(constrained_df)/len(df))\n",
    "    \n",
    "    #immobilized_df = df[(df['alpha'] <= 0.1)]\n",
    "    #immobilized_percent[i] = (len(immobilized_df)/len(df))\n",
    "    \n",
    "    \n",
    "#plt.bar(labels, immobilized_percent, color='r', label='immobilized')\n",
    "bar_w = 0.5\n",
    "plt.bar(labels, constrained_percent, label='Subdiffusive', width=bar_w, color='grey')\n",
    "plt.bar(labels, normal_percent, bottom=constrained_percent+immobilized_percent, color='#b7a57a', label='Brownian', width=bar_w)\n",
    "plt.bar(labels, directed_percent, bottom=constrained_percent+immobilized_percent+normal_percent, color='#4b2e83', label='Superdiffusive', width=bar_w)\n",
    "plt.ylim([0,1])\n",
    "plt.legend()\n",
    "plt.title(f'Percentage of Diffusion Modes per {target}')\n",
    "plt.ylim([0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
  },
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
