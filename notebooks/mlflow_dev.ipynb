{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diff_predictor import data_process, predxgboost, spatial\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "from os import listdir, getcwd, chdir\n",
    "from os.path import isfile, join\n",
    "import os\n",
    "from sklearn.preprocessing import scale, StandardScaler\n",
    "from numpy.random import permutation\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, recall_score, precision_score, f1_score\n",
    "import operator\n",
    "import xgboost as xgb\n",
    "import shap\n",
    "from xgboost.training import CVPack\n",
    "from xgboost import callback\n",
    "from xgboost.core import CallbackEnv\n",
    "from xgboost.core import EarlyStopException\n",
    "from xgboost.core import STRING_TYPES\n",
    "\n",
    "import mlflow\n",
    "import warnings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Notebook Dir: /Users/nelsschimek/Documents/nancelab/diff_predictor/notebooks\n",
      "Using current directory for loading data: /Users/nelsschimek/Documents/nancelab/diff_predictor\n"
     ]
    }
   ],
   "source": [
    "workbookDir = getcwd()\n",
    "\n",
    "print('Current Notebook Dir: ' + workbookDir)\n",
    "chdir(workbookDir) # Go to current workbook Dir\"\n",
    "chdir('..')        # Go up one\n",
    "print(f'Using current directory for loading data: {getcwd()}')\n",
    "workbookDir = getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "['features_P10F_5uM_7DIV_40nm_slice_1_cortex_vid_1.csv', 'features_P10F_NT_7DIV_40nm_slice_1_cortex_vid_1.csv', 'features_P10F_NT_7DIV_40nm_slice_1_cortex_vid_2.csv', 'features_P10F_1uM_7DIV_40nm_slice_1_cortex_vid_1.csv', 'features_P10F_50nM_7DIV_40nm_slice_1_cortex_vid_2.csv', 'features_P10F_5uM_7DIV_40nm_slice_2_cortex_vid_2.csv', 'features_P10F_1uM_7DIV_40nm_slice_1_cortex_vid_2.csv', 'features_P10F_50nM_7DIV_40nm_slice_1_cortex_vid_1.csv']\n"
     ]
    }
   ],
   "source": [
    "rotenone_feature_path = workbookDir + '/data/Brendan_traj_data/Cortex_features/'\n",
    "rotenone_feature_filelist = [f for f in listdir(rotenone_feature_path) if isfile(join(rotenone_feature_path, f)) and 'feat' in f and '7DIV' in f ]\n",
    "print(len(rotenone_feature_filelist))\n",
    "print(rotenone_feature_filelist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding file features_P10F_5uM_7DIV_40nm_slice_1_cortex_vid_1.csv size: (2405, 68)\n",
      "Adding file features_P10F_NT_7DIV_40nm_slice_1_cortex_vid_1.csv size: (1738, 68)\n",
      "Adding file features_P10F_NT_7DIV_40nm_slice_1_cortex_vid_2.csv size: (1190, 68)\n",
      "Adding file features_P10F_1uM_7DIV_40nm_slice_1_cortex_vid_1.csv size: (638, 68)\n",
      "Adding file features_P10F_50nM_7DIV_40nm_slice_1_cortex_vid_2.csv size: (1785, 68)\n",
      "Adding file features_P10F_5uM_7DIV_40nm_slice_2_cortex_vid_2.csv size: (2829, 68)\n",
      "Adding file features_P10F_1uM_7DIV_40nm_slice_1_cortex_vid_2.csv size: (863, 68)\n",
      "Adding file features_P10F_50nM_7DIV_40nm_slice_1_cortex_vid_1.csv size: (2632, 68)\n"
     ]
    }
   ],
   "source": [
    "fstats_tot_rotenone = data_process.generate_fullstats(rotenone_feature_path, rotenone_feature_filelist, ['NT','50nM', '1uM', '5uM'], 'dosage')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_list = [\n",
    "    'alpha', # Fitted anomalous diffusion alpha exponenet\n",
    "    'D_fit', # Fitted anomalous diffusion coefficient\n",
    "    'kurtosis', # Kurtosis of track\n",
    "    'asymmetry1', # Asymmetry of trajecory (0 for circular symmetric, 1 for linear)\n",
    "    'asymmetry2', # Ratio of the smaller to larger principal radius of gyration\n",
    "    'asymmetry3', # An asymmetric feature that accnts for non-cylindrically symmetric pt distributions\n",
    "    'AR', # Aspect ratio of long and short side of trajectory's minimum bounding rectangle\n",
    "    'elongation', # Est. of amount of extension of trajectory from centroid\n",
    "    'boundedness', # How much a particle with Deff is restricted by a circular confinement of radius r\n",
    "    'fractal_dim', # Measure of how complicated a self similar figure is\n",
    "    'trappedness', # Probability that a particle with Deff is trapped in a region\n",
    "    'efficiency', # Ratio of squared net displacement to the sum of squared step lengths\n",
    "    'straightness', # Ratio of net displacement to the sum of squared step lengths\n",
    "    'MSD_ratio', # MSD ratio of the track\n",
    "#     'frames', # Number of frames the track spans\n",
    "    'Deff1', # Effective diffusion coefficient at 0.33 s\n",
    "    'Deff2', # Effective diffusion coefficient at 3.3 s\n",
    "    #'angle_mean', # Mean turning angle which is counterclockwise angle from one frame point to another\n",
    "    #'angle_mag_mean', # Magnitude of the turning angle mean\n",
    "    #'angle_var', # Variance of the turning angle\n",
    "    #'dist_tot', # Total distance of the trajectory\n",
    "    #'dist_net', # Net distance from first point to last point\n",
    "    #'progression', # Ratio of the net distance traveled and the total distance\n",
    "    'Mean alpha', \n",
    "    'Mean D_fit', \n",
    "    'Mean kurtosis', \n",
    "    'Mean asymmetry1', \n",
    "    'Mean asymmetry2',\n",
    "    'Mean asymmetry3', \n",
    "    'Mean AR',\n",
    "    'Mean elongation', \n",
    "    'Mean boundedness',\n",
    "    'Mean fractal_dim', \n",
    "    'Mean trappedness', \n",
    "    'Mean efficiency',\n",
    "    'Mean straightness', \n",
    "    'Mean MSD_ratio', \n",
    "    'Mean Deff1', \n",
    "    'Mean Deff2',\n",
    "    ]\n",
    "\n",
    "target = 'dosage'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14080, 36)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(12493, 36)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ecm = fstats_tot_rotenone[feature_list + [target, 'Track_ID', 'X', 'Y']] #dont think i need these rn\n",
    "print(ecm.shape)\n",
    "ecm = ecm[~ecm[list(set(feature_list) - set(['Deff2', 'Mean Deff2']))].isin([np.nan, np.inf, -np.inf]).any(1)]       # Removing nan and inf data points\n",
    "ecm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wine Quality Sample\n",
    "def full_preprocess(ecm, balanced=True, y_scramble=False, target=None):\n",
    "\n",
    "    rand_state = np.random.randint(1, 2000)\n",
    "    if balanced:\n",
    "        bal_ecm = data_process.balance_data(ecm, target, random_state=rand_state)\n",
    "        bal_ecm = bal_ecm.reset_index(drop=True)\n",
    "        #sampled_df = bal_ecm.sample(frac=0.5)\n",
    "        sampled_df = data_process.bin_data(bal_ecm)\n",
    "    else:\n",
    "        sampled_df = data_process.bin_data(ecm)\n",
    "    label_df = sampled_df[target]\n",
    "    features_df = sampled_df.drop([target, 'X', 'Y', 'binx', 'biny', 'bins', 'Track_ID'], axis=1)\n",
    "    features = features_df.columns\n",
    "\n",
    "    if y_scramble:\n",
    "        perm = permutation(len(label_df))\n",
    "        label_shuffled = label_df[perm]\n",
    "        le = preprocessing.LabelEncoder()\n",
    "        sampled_df['encoded_target'] = le.fit_transform(label_shuffled)\n",
    "    else:\n",
    "        le = preprocessing.LabelEncoder()\n",
    "        sampled_df['encoded_target'] = le.fit_transform(sampled_df[target])\n",
    "\n",
    "    seed = rand_state\n",
    "    np.random.seed(seed)\n",
    "    train_split = 0.8\n",
    "    test_split = 0.5\n",
    "\n",
    "\n",
    "    training_bins = np.random.choice(sampled_df['bins'].unique(), int(len(sampled_df['bins'].unique())*train_split), replace=False)\n",
    "\n",
    "    X_train = sampled_df[sampled_df['bins'].isin(training_bins)]\n",
    "    X_test_val = sampled_df[~sampled_df['bins'].isin(training_bins)]\n",
    "    X_val, X_test = train_test_split(X_test_val, test_size=test_split, random_state=seed)\n",
    "\n",
    "    y_train = X_train['encoded_target']\n",
    "    y_test = X_test['encoded_target']\n",
    "    y_val = X_val['encoded_target']\n",
    "\n",
    "    dtrain = xgb.DMatrix(X_train[features], label=y_train)\n",
    "    dtest = xgb.DMatrix(X_test[features], label=y_test)\n",
    "    dval = xgb.DMatrix(X_val[features], label=y_val)\n",
    "\n",
    "    \n",
    "    \n",
    "    import logging\n",
    "    logging.basicConfig(level=logging.WARN)\n",
    "    logger = logging.getLogger(__name__)\n",
    "\n",
    "    def eval_metrics(actual, pred):\n",
    "        \n",
    "        return rmse, mae, r2\n",
    "\n",
    "\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    np.random.seed(40)\n",
    "\n",
    "    \n",
    "\n",
    "    # Set default values if no alpha is provided\n",
    "    # if float(in_alpha) is None:\n",
    "    #     alpha = 0.5\n",
    "    # else:\n",
    "    #     alpha = float(in_alpha)\n",
    "\n",
    "    # Set default values if no l1_ratio is provided\n",
    "    param = {'max_depth': 3,\n",
    "         'eta': 0.005,\n",
    "         'min_child_weight': 0,\n",
    "         'verbosity': 0,\n",
    "         'objective': 'multi:softprob',\n",
    "         'num_class': 4,\n",
    "         'silent': 'True',\n",
    "         'gamma': 5,\n",
    "         'subsample': 0.15,\n",
    "         'colsample_bytree': 0.8,\n",
    "         'eval_metric': \"mlogloss\",\n",
    "#          # GPU integration will cut time in ~half:\n",
    "#          'gpu_id' : 0,\n",
    "#          'tree_method': 'gpu_hist',\n",
    "#          'predictor': 'gpu_predictor'\n",
    "         }\n",
    "\n",
    "    # Useful for multiple runs (only doing one run in this sample notebook)    \n",
    "    with mlflow.start_run():\n",
    "        # Execute ElasticNet\n",
    "        booster, acc, true_label, preds = predxgboost.train(param, dtrain, dtest, dval, evals=[(dtrain, 'train'), (dval, 'eval')], num_round=200, verbose=False)\n",
    "\n",
    "\n",
    "        # Evaluate Metrics\n",
    "        # predicted_qualities = booster.predict(dv)\n",
    "        # (rmse, mae, r2) = eval_metrics(test_y, predicted_qualities)\n",
    "\n",
    "        # Print out metrics\n",
    "        #print(\"Elasticnet model (alpha=%f, l1_ratio=%f):\" % (alpha, l1_ratio))\n",
    "        print(\"  Accuracy: %s\" % acc)\n",
    "        \n",
    "\n",
    "        # Log parameter, metrics, and model to MLflow\n",
    "        #mlflow.log_param(\"alpha\", alpha)\n",
    "        mlflow.log_metric(\"Accuracy\", acc)\n",
    "        \n",
    "\n",
    "        mlflow.xgboost.log_model(booster, \"model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ratio before data balance (5uM:NT:1uM:50nM) = 4577:2759:1289:3868\n",
      "Ratio after balance (5uM:NT:1uM:50nM) = 1289:1289:1289:1289\n",
      "Accuracy: 0.47947761194029853\n",
      "  Accuracy: 0.47947761194029853\n"
     ]
    }
   ],
   "source": [
    "full_preprocess(ecm, target='dosage')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
