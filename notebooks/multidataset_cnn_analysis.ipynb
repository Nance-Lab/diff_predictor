{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import normalize\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Dense,BatchNormalization,Conv2D, Conv1D\n",
    "from keras.layers import Input,GlobalMaxPooling2D,concatenate, GlobalMaxPooling1D, Flatten, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping,ReduceLROnPlateau,ModelCheckpoint\n",
    "from os import listdir, getcwd, chdir\n",
    "from os.path import isfile, join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workbookDir = getcwd()\n",
    "print('Current Notebook Dir: ' + workbookDir)\n",
    "chdir(workbookDir) # Go to current workbook Dir\"\n",
    "chdir('..')        # Go up one\n",
    "print(f'Using current directory for loading data: {getcwd()}')\n",
    "workbookDir = getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../diff_predictor/data/cortex_mpt_data_nonans/'\n",
    "filelist = [f for f in listdir(data_path) if isfile(join(data_path, f)) \n",
    "    #and 'P10F' in f\n",
    "    and 'ChABC' not in f\n",
    "    and 'HYase' not in f\n",
    "    and 'msd_NT' not in f]\n",
    "    # and 'P21' not in f\n",
    "    # and 'P28' not in f]\n",
    "\n",
    "# print('not treated datasets')\n",
    "p14_filelist = [f for f in listdir(data_path) if isfile(join(data_path, f)) and 'P14' in f]\n",
    "p14_count = len(p14_filelist)\n",
    "print(len(p14_filelist))\n",
    "p21_filelist = [f for f in listdir(data_path) if isfile(join(data_path, f)) and 'P21' in f]\n",
    "p21_count = len(p21_filelist)\n",
    "print(len(p21_filelist))\n",
    "p28_filelist = [f for f in listdir(data_path) if isfile(join(data_path, f)) and 'P28' in f]\n",
    "p28_count = len(p28_filelist)\n",
    "print(len(p28_filelist))\n",
    "p35_filelist = [f for f in listdir(data_path) if isfile(join(data_path, f)) and 'P35' in f]\n",
    "p35_count = len(p35_filelist)\n",
    "print(len(p35_filelist))\n",
    "p70_filelist = [f for f in listdir(data_path) if isfile(join(data_path, f)) and 'P70' in f]\n",
    "p70_count = len(p70_filelist)\n",
    "print(len(p70_filelist))\n",
    "\n",
    "treatment_nt_filelist = [f for f in listdir(data_path) if isfile(join(data_path, f)) and 'msd_NT' in f]\n",
    "print(len(treatment_nt_filelist))\n",
    "\n",
    "\n",
    "# # # Will need some extra stuff\n",
    "region_cortex_filelist = [f for f in listdir(data_path) if isfile(join(data_path, f)) and 'cortex' in f and 'P10F' not in f]\n",
    "region_count = len(region_cortex_filelist)\n",
    "print(len(region_cortex_filelist))\n",
    "\n",
    "\n",
    "NT_filelist = [f for f in listdir(data_path) if isfile(join(data_path, f)) and 'P10F_NT' in f]\n",
    "nt_count = len(NT_filelist)\n",
    "print(len(NT_filelist))\n",
    "print()\n",
    "# print('treaated datasets')\n",
    "\n",
    "chabc_filelist = [f for f in listdir(data_path) if isfile(join(data_path, f) and 'ChABC' in f)]\n",
    "print(len(chabc_filelist))\n",
    "hyase_filelist = [f for f in listdir(data_path) if isfile(join(data_path, f) and 'HYase' in f)]\n",
    "print(len(hyase_filelist))\n",
    "\n",
    "oneuM_filelist = [f for f in listdir(data_path) if isfile(join(data_path, f)) and '1uM' in f]\n",
    "oneum_count = len(oneuM_filelist)\n",
    "print(len(oneuM_filelist))\n",
    "fiveuM_filelist = [f for f in listdir(data_path) if isfile(join(data_path, f)) and '5uM' in f]\n",
    "fiveum_count = len(fiveuM_filelist)\n",
    "print(len(fiveuM_filelist))\n",
    "tenuM_filelist = [f for f in listdir(data_path) if isfile(join(data_path, f)) and '10uM' in f]\n",
    "tenum_count = len(tenuM_filelist)\n",
    "print(len(tenuM_filelist))\n",
    "fiftynM_filelist = [f for f in listdir(data_path) if isfile(join(data_path, f)) and '50nM' in f]\n",
    "fiftynm_count = len(fiftynM_filelist)\n",
    "print(len(fiftynM_filelist))\n",
    "\n",
    "\n",
    "\n",
    "print(len(filelist)) #Check to make sure there are the right number of files, in this case 180 - 60 per class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tenuM_filelist = [f for f in listdir(data_path) if isfile(join(data_path, f)) and '10uM' in f]\n",
    "tenum_count = len(tenuM_filelist)\n",
    "print(len(tenuM_filelist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nontreated_tot = p14_count+p21_count+p28_count+p35_count+p70_count+region_count+nt_count\n",
    "print(nontreated_tot)\n",
    "\n",
    "treated_tot = fiftynm_count+oneum_count+fiveum_count+tenum_count\n",
    "print(treated_tot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('not treated datasets')\n",
    "\n",
    "p14_training_filelist = list(np.random.choice(p14_filelist, size=int(len(p14_filelist)*0.8), replace=False))\n",
    "p14_validation_filelist = [f for f in p14_filelist if f not in p14_training_filelist]\n",
    "\n",
    "p21_training_filelist = list(np.random.choice(p21_filelist, size=int(len(p21_filelist)*0.8), replace=False))\n",
    "p21_validation_filelist = [f for f in p21_filelist if f not in p21_training_filelist]\n",
    "\n",
    "p28_training_filelist = list(np.random.choice(p28_filelist, size=int(len(p28_filelist)*0.8), replace=False))\n",
    "p28_validation_filelist = [f for f in p28_filelist if f not in p28_training_filelist]\n",
    "\n",
    "\n",
    "p35_training_filelist = list(np.random.choice(p35_filelist, size=int(len(p35_filelist)*0.8), replace=False))\n",
    "p35_validation_filelist = [f for f in p35_filelist if f not in p35_training_filelist]\n",
    "\n",
    "p70_training_filelist = list(np.random.choice(p70_filelist, size=int(len(p70_filelist)*0.8), replace=False))\n",
    "p70_validation_filelist = [f for f in p70_filelist if f not in p70_training_filelist]\n",
    "\n",
    "# treatment_nt_filelist = [f for f in listdir(data_path) if isfile(join(data_path, f)) and 'msd_NT' in f]\n",
    "# print(len(treatment_nt_filelist))\n",
    "\n",
    "\n",
    "# # # # Will need some extra stuff\n",
    "\n",
    "region_training_filelist = list(np.random.choice(region_cortex_filelist, size=int(len(region_cortex_filelist)*0.8), replace=False))\n",
    "region_validation_filelist = [f for f in region_cortex_filelist if f not in region_training_filelist]\n",
    "\n",
    "\n",
    "nt_training_filelist = list(np.random.choice(NT_filelist, size=int(len(NT_filelist)*0.8), replace=False))\n",
    "nt_validation_filelist = [f for f in NT_filelist if f not in nt_training_filelist]\n",
    "\n",
    "oneum_training_filelist = list(np.random.choice(oneuM_filelist, size=int(len(oneuM_filelist)*0.8), replace=False))\n",
    "oneum_validation_filelist = [f for f in oneuM_filelist if f not in oneum_training_filelist]\n",
    "\n",
    "fiveum_training_filelist = list(np.random.choice(fiveuM_filelist, size=int(len(fiveuM_filelist)*0.8), replace=False))\n",
    "fiveum_validation_filelist = [f for f in fiveuM_filelist if f not in fiveum_training_filelist]\n",
    "\n",
    "tenum_training_filelist = list(np.random.choice(tenuM_filelist, size=int(len(tenuM_filelist)*0.8), replace=False))\n",
    "tenum_validation_filelist = [f for f in tenuM_filelist if f not in tenum_training_filelist]\n",
    "\n",
    "fiftynm_training_filelist = list(np.random.choice(fiftynM_filelist, size=int(len(fiftynM_filelist)*0.8), replace=False))\n",
    "fiftynm_validation_filelist = [f for f in fiftynM_filelist if f not in fiftynm_training_filelist]\n",
    "\n",
    "# fiftynM_filelist = [f for f in listdir(data_path) if isfile(join(data_path, f)) and '50nM' in f]\n",
    "# fiftynm_count = len(fiftynM_filelist)\n",
    "# print(len(fiftynM_filelist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tenum_training_filelist = list(np.random.choice(tenuM_filelist, size=int(len(tenuM_filelist)*0.8), replace=False))\n",
    "tenum_validation_filelist = [f for f in tenuM_filelist if f not in tenum_training_filelist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p14_training_filelist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nontreated_filelist_training = nt_training_filelist #+ region_training_filelist#p14_training_filelist+ p21_training_filelist + p28_training_filelist + p35_training_filelist + p70_training_filelist + region_training_filelist + nt_training_filelist\n",
    "treated_filelist_training = fiftynm_training_filelist#+oneum_training_filelist+fiveum_training_filelist+tenum_training_filelist\n",
    "print(len(nontreated_filelist_training))\n",
    "print(len(treated_filelist_training))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "downsampled_treated_filelist_training = list(np.random.choice(treated_filelist_training, len(nontreated_filelist_training), replace=False))\n",
    "print(len(downsampled_treated_filelist_training))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nontreated_filelist_validation = nt_validation_filelist#+region_validation_filelist#p14_validation_filelist+ p21_validation_filelist + p28_validation_filelist + p35_validation_filelist + p70_validation_filelist + region_validation_filelist + nt_validation_filelist\n",
    "treated_filelist_validation = fiftynm_validation_filelist#+oneum_validation_filelist+fiveum_validation_filelist+tenum_validation_filelist\n",
    "print(len(nontreated_filelist_validation))\n",
    "print(len(treated_filelist_validation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "downsampled_treated_filelist_validation = list(np.random.choice(treated_filelist_validation, len(nontreated_filelist_validation), replace=False))\n",
    "print(len(downsampled_treated_filelist_validation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "downsampled_p70_filelist_training = list(np.random.choice(p70_training_filelist, len(p14_training_filelist), replace=False))\n",
    "print(len(downsampled_p70_filelist_training))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "downsampled_p70_filelist_validation= list(np.random.choice(p70_validation_filelist, len(p14_validation_filelist), replace=False))\n",
    "print(len(downsampled_p70_filelist_validation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(p14_validation_filelist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partition = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_ids = []\n",
    "training_labels = dict()\n",
    "for file in (p14_training_filelist + downsampled_p70_filelist_training):\n",
    "\n",
    "    label = file[:-4]\n",
    "    if \"14\" in file:\n",
    "        training_labels[str(label)] = 1\n",
    "    \n",
    "    else:\n",
    "        training_labels[str(label)] = 0\n",
    "    track_ids.append(str(label))\n",
    "        #np.save(str(f'data/10div_train_min50steps/{file}').replace(\".csv\", f\"_track_{int(track_id)}\"), test_array)\n",
    "\n",
    "partition['train'] = track_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_track_ids = []\n",
    "validation_labels = dict()\n",
    "for file in (p14_validation_filelist + downsampled_p70_filelist_validation):\n",
    "\n",
    "    label = file[:-4]\n",
    "    if \"14\" in file:\n",
    "        validation_labels[str(label)] = 1\n",
    "    \n",
    "    else:\n",
    "        validation_labels[str(label)] = 0\n",
    "    track_ids.append(str(label))\n",
    "        #np.save(str(f'data/10div_train_min50steps/{file}').replace(\".csv\", f\"_track_{int(track_id)}\"), test_array)\n",
    "\n",
    "partition['validation'] = validation_track_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up training files and labels\n",
    "min = 10000\n",
    "max = 0\n",
    "\n",
    "track_ids = []\n",
    "training_labels = dict()\n",
    "for file in (downsampled_treated_filelist_training + nontreated_filelist_training):\n",
    "\n",
    "\n",
    "    \n",
    "#     track_data = df[df['Track_ID']==track_id][['X', 'Y']]\n",
    "#     track_data = track_data.fillna(0)\n",
    "# #     #dataset[str(track_id)] = np.array(track_data)\n",
    "\n",
    "#     test_array = np.array(track_data)\n",
    "    #norm_array = normalize(test_array)\n",
    "    # print(np.count_nonzero(test_array))\n",
    "    # lengths.append(len(test_array))\n",
    "    #print(np.count_nonzero(np.isnan(test_array)))\n",
    "    #if np.count_nonzero(test_array) >200:\n",
    "        # print(np.count_nonzero(test_array))\n",
    "    #out_array = np.load(data_path + file)\n",
    "    # if np.min(out_array) < min:\n",
    "    #     min = np.min(out_array)\n",
    "    # if np.max(out_array) > max:\n",
    "    #     max = np.max(out_array)\n",
    "\n",
    "    #no_zeros = np.argwhere(out_array)\n",
    "    #print(len(no_zeros))\n",
    "    #if len(out_array) >=100:\n",
    "    label = file[:-4]\n",
    "    if \"uM\" in file:\n",
    "        training_labels[str(label)] = 1\n",
    "    elif \"nM\" in file:\n",
    "        training_labels[str(label)] = 1\n",
    "    elif \"ChABC\" in file:\n",
    "        training_labels[str(label)] = 1\n",
    "    elif \"HYase\" in file:\n",
    "        training_labels[str(label)] = 1\n",
    "    else:\n",
    "        training_labels[str(label)] = 0\n",
    "    track_ids.append(str(label))\n",
    "        #np.save(str(f'data/10div_train_min50steps/{file}').replace(\".csv\", f\"_track_{int(track_id)}\"), test_array)\n",
    "\n",
    "partition['train'] = track_ids\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up training files and labels\n",
    "min = 10000\n",
    "max = 0\n",
    "\n",
    "validation_track_ids = []\n",
    "validation_labels = dict()\n",
    "for file in (downsampled_treated_filelist_validation + nontreated_filelist_validation):\n",
    "\n",
    "\n",
    "    \n",
    "#     track_data = df[df['Track_ID']==track_id][['X', 'Y']]\n",
    "#     track_data = track_data.fillna(0)\n",
    "# #     #dataset[str(track_id)] = np.array(track_data)\n",
    "\n",
    "#     test_array = np.array(track_data)\n",
    "    #norm_array = normalize(test_array)\n",
    "    # print(np.count_nonzero(test_array))\n",
    "    # lengths.append(len(test_array))\n",
    "    #print(np.count_nonzero(np.isnan(test_array)))\n",
    "    #if np.count_nonzero(test_array) >200:\n",
    "        # print(np.count_nonzero(test_array))\n",
    "    #out_array = np.load(data_path + file)\n",
    "    # if np.min(out_array) < min:\n",
    "    #     min = np.min(out_array)\n",
    "    # if np.max(out_array) > max:\n",
    "    #     max = np.max(out_array)\n",
    "\n",
    "    #no_zeros = np.argwhere(out_array)\n",
    "    #print(len(no_zeros))\n",
    "    #if len(out_array) >=100:\n",
    "    label = file[:-4]\n",
    "    if \"uM\" in file:\n",
    "        validation_labels[str(label)] = 1\n",
    "    elif \"nM\" in file:\n",
    "        validation_labels[str(label)] = 1\n",
    "    elif \"ChABC\" in file:\n",
    "        validation_labels[str(label)] = 1\n",
    "    elif \"HYase\" in file:\n",
    "        validation_labels[str(label)] = 1\n",
    "    else:\n",
    "        validation_labels[str(label)] = 0\n",
    "    validation_track_ids.append(str(label))\n",
    "        #np.save(str(f'data/10div_train_min50steps/{file}').replace(\".csv\", f\"_track_{int(track_id)}\"), test_array)\n",
    "\n",
    "partition['validation'] = validation_track_ids\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_array = out_array/max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(out_array[:,0], out_array[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(scaled_array[:,0], scaled_array[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_filelist = one_um_filelist[3:] + nt_filelist[3:]\n",
    "print(len(validation_filelist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up validation files and labels\n",
    "\n",
    "track_ids = []\n",
    "validation_labels = dict()\n",
    "for file in validation_filelist:\n",
    "\n",
    "    df = pd.read_csv(data_path + file)\n",
    "\n",
    "    for track_id in df['Track_ID'].unique():\n",
    "        label = file[:-4] + f'_track_{int(track_id)}'\n",
    "        if \"NT\" in file:\n",
    "            validation_labels[str(label)] = 1\n",
    "        else:\n",
    "            validation_labels[str(label)] = 0\n",
    "        \n",
    "        track_data = df[df['Track_ID']==track_id][['X', 'Y']]\n",
    "        track_data = track_data.fillna(0)\n",
    "    #     #dataset[str(track_id)] = np.array(track_data)\n",
    "        test_array = np.array(track_data)\n",
    "        #norm_array = normalize(test_array)\n",
    "        if np.count_nonzero(test_array) >200:\n",
    "            track_ids.append(str(label))\n",
    "            #np.save(str(f'data/10div_val_min50steps/{file}').replace(\".csv\", f\"_track_{int(track_id)}\"), test_array)\n",
    "\n",
    "partition['validation'] = track_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_array[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_data_path = '../diff_predictor/data/10div_training_data/'\n",
    "one_um_filelist = [f for f in listdir(cnn_data_path) if isfile(join(cnn_data_path, f)) and '1uM' in f]\n",
    "\n",
    "nt_filelist = [f for f in listdir(cnn_data_path) if isfile(join(cnn_data_path, f)) and 'NT' in f]\n",
    "len(one_um_filelist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = out[out!=0]\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(5, 5, figsize=(10,10))\n",
    "for i in range(5):\n",
    "    for j in range(5):\n",
    "        file = np.load(cnn_data_path+one_um_filelist[counter])\n",
    "        if np.count_nonzero(file) >0:\n",
    "            counter += 1\n",
    "            axes[i,j].plot(file[:,0], file[:,1])\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(5, 5, figsize=(10,10))\n",
    "for i in range(5):\n",
    "    for j in range(5):\n",
    "        file = np.load(cnn_data_path+nt_filelist[counter])\n",
    "        counter += 1\n",
    "        axes[i,j].plot(file[:,0], file[:,1])\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partition.keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing_data_generation(data_path, list_IDs_temp, labels, batch_size, dim=(651,2)):\n",
    "    'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "    # Initialization\n",
    "    X = np.empty(dim)#, self.n_channels))\n",
    "    y = np.empty(dtype=int)\n",
    "\n",
    "    # Generate data\n",
    "    for i, ID in enumerate(list_IDs_temp):\n",
    "        # Store sample\n",
    "        out_array = np.load(data_path + ID + '.npy')\n",
    "        out_array = normalize(out_array)\n",
    "        X = out_array\n",
    "\n",
    "        # Store class\n",
    "        y[i] = labels[ID]\n",
    "\n",
    "    return X, keras.utils.to_categorical(y, num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = testing_data_generation('../diff_predictor/data/cortex_mpt_datasets/', partition['train'], training_labels, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MptDataGenerator(keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, data_path, list_IDs, labels, batch_size=64, dim=(651,2), n_channels=None,\n",
    "                 n_classes=2, shuffle=True):\n",
    "        'Initialization'\n",
    "        self.dim = dim\n",
    "        self.batch_size = batch_size\n",
    "        self.labels = labels\n",
    "        self.list_IDs = list_IDs\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.shuffle = shuffle\n",
    "        self.data_path = data_path\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "        # Find list of IDs\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(list_IDs_temp)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.list_IDs))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "        # Initialization\n",
    "        X = np.zeros((self.batch_size, *self.dim))#, self.n_channels))\n",
    "        y = np.zeros((self.batch_size), dtype=int)\n",
    "\n",
    "        # Generate data\n",
    "        for i, ID in enumerate(list_IDs_temp):\n",
    "            # Store sample\n",
    "            out_array = np.load(self.data_path + ID + '.npy')\n",
    "            out_array = out_array[:self.dim[0], :self.dim[1]]\n",
    "            out_array_scaled = out_array[:-1] - out_array[1:]\n",
    "            #out_array_scaled = out_array/2048.0\n",
    "            #out_array = normalize(out_array)\n",
    "            X[i, 0:len(out_array_scaled), 0:2] = out_array_scaled\n",
    "\n",
    "            # Store class\n",
    "            y[i] = self.labels[ID]\n",
    "\n",
    "        return X, keras.utils.to_categorical(y, num_classes=self.n_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_confmatrix_xys(list_ids, data_path, labels, dim, num_classes):\n",
    "    X = np.zeros((len(list_ids), *dim))#, self.n_channels))\n",
    "    y = np.zeros(len(list_ids), dtype=int)\n",
    "    for i, ID in enumerate(list_ids):\n",
    "            # Store sample\n",
    "            out_array = np.load(data_path + ID + '.npy')\n",
    "            out_array = out_array[:dim[0], :dim[1]]\n",
    "            out_array_scaled = out_array[:-1] - out_array[1:]\n",
    "            #out_array_scaled = out_array/2048.0\n",
    "            #out_array = normalize(out_array)\n",
    "            X[i, 0:len(out_array_scaled), 0:2] = out_array_scaled\n",
    "\n",
    "            # Store class\n",
    "            y[i] = labels[ID]\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = get_confmatrix_xys(list_ids=partition['validation'], data_path='../diff_predictor/data/cortex_mpt_data_nonans/', labels=validation_labels, dim=(651,2), num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_array = np.array([5, 4, 3, 2, 1])\n",
    "subbed_arra = test_array[:-1] - test_array[1:]\n",
    "subbed_arra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_generator = MptDataGenerator('../diff_predictor/data/cortex_mpt_data_nonans/', partition['train'], training_labels, batch_size=32, dim=(651,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_generator = MptDataGenerator('../diff_predictor/data/cortex_mpt_data_nonans/', partition['validation'], validation_labels, batch_size=32, dim=(651,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = Input((651, 2))\n",
    "initializer = 'he_normal'\n",
    "f = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = Conv1D(f,4,padding='causal',activation='relu',kernel_initializer=initializer)(inputs)\n",
    "x1 = BatchNormalization()(x1)\n",
    "x1 = Conv1D(f,4,dilation_rate=2,padding='causal',activation='relu',kernel_initializer=initializer)(x1)\n",
    "x1 = BatchNormalization()(x1)\n",
    "x1 = Conv1D(f,4,dilation_rate=4,padding='causal',activation='relu',kernel_initializer=initializer)(x1)\n",
    "x1 = BatchNormalization()(x1)\n",
    "x1 = GlobalMaxPooling1D()(x1)\n",
    "\n",
    "dense = Dense(512,activation='relu')(x1)\n",
    "dense = Dense(256,activation='relu')(dense)\n",
    "dense2 = Dense(2,activation='softmax')(dense)\n",
    "model = Model(inputs=inputs, outputs=dense2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='mse',\n",
    "              optimizer=Adam(lr=1e-5),\n",
    "              metrics=['mse', 'accuracy', 'binary_accuracy'])\n",
    "\n",
    "history = model.fit(training_generator, epochs=50, steps_per_epoch=100, validation_data=validation_generator, validation_steps=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(history.history.keys())\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "Y_pred = model.predict(x, )\n",
    "y_pred = np.argmax(Y_pred, axis=1)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "cm = confusion_matrix(y, y_pred)\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Rot', 'NT'])\n",
    "\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = Conv1D(f,4,padding='causal',activation='relu',kernel_initializer=initializer)(inputs)\n",
    "x1 = BatchNormalization()(x1)\n",
    "x1 = Conv1D(f,4,dilation_rate=2,padding='causal',activation='relu',kernel_initializer=initializer)(x1)\n",
    "x1 = BatchNormalization()(x1)\n",
    "x1 = Conv1D(f,4,dilation_rate=4,padding='causal',activation='relu',kernel_initializer=initializer)(x1)\n",
    "x1 = BatchNormalization()(x1)\n",
    "x1 = GlobalMaxPooling1D()(x1)\n",
    "\n",
    "\n",
    "x2 = Conv1D(f,2,padding='causal',activation='relu',kernel_initializer=initializer)(inputs)\n",
    "x2 = BatchNormalization()(x2)\n",
    "x2 = Conv1D(f,2,dilation_rate=2,padding='causal',activation='relu',kernel_initializer=initializer)(x2)\n",
    "x2 = BatchNormalization()(x2)\n",
    "x2 = Conv1D(f,2,dilation_rate=4,padding='causal',activation='relu',kernel_initializer=initializer)(x2)\n",
    "x2 = BatchNormalization()(x2)\n",
    "x2 = GlobalMaxPooling1D()(x2)\n",
    "\n",
    "\n",
    "x3 = Conv1D(f,3,padding='causal',activation='relu',kernel_initializer=initializer)(inputs)\n",
    "x3 = BatchNormalization()(x3)\n",
    "x3 = Conv1D(f,3,dilation_rate=2,padding='causal',activation='relu',kernel_initializer=initializer)(x3)\n",
    "x3 = BatchNormalization()(x3)\n",
    "x3 = Conv1D(f,3,dilation_rate=4,padding='causal',activation='relu',kernel_initializer=initializer)(x3)\n",
    "x3 = BatchNormalization()(x3)\n",
    "x3 = GlobalMaxPooling1D()(x3)\n",
    "\n",
    "\n",
    "x4 = Conv1D(f,10,padding='causal',activation='relu',kernel_initializer=initializer)(inputs)\n",
    "x4 = BatchNormalization()(x4)\n",
    "x4 = Conv1D(f,10,dilation_rate=4,padding='causal',activation='relu',kernel_initializer=initializer)(x4)\n",
    "x4 = BatchNormalization()(x4)\n",
    "x4 = Conv1D(f,10,dilation_rate=8,padding='causal',activation='relu',kernel_initializer=initializer)(x4)\n",
    "x4 = BatchNormalization()(x4)\n",
    "x4 = GlobalMaxPooling1D()(x4)\n",
    "\n",
    "\n",
    "x5 = Conv1D(f,20,padding='same',activation='relu',kernel_initializer=initializer)(inputs)\n",
    "x5 = BatchNormalization()(x5)\n",
    "x5 = GlobalMaxPooling1D()(x5)\n",
    "\n",
    "\n",
    "con = concatenate([x1,x2,x3,x4,x5])\n",
    "dense = Dense(1024,activation='relu')(con)\n",
    "dense = Dense(512,activation='relu')(dense)\n",
    "dense = Dense(256,activation='relu')(dense)\n",
    "dense = Dense(128,activation='relu')(dense)\n",
    "dense2 = Dense(2,activation='softmax')(dense)\n",
    "model = Model(inputs=inputs, outputs=dense2)\n",
    "\n",
    "##optimizer = Adam(lr=1e-5)\n",
    "#model.compile(optimizer=optimizer,loss='categorical_crossentropy',metrics=['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = Conv1D(f,4,padding='causal',activation='relu',kernel_initializer=initializer)(inputs)\n",
    "x1 = BatchNormalization()(x1)\n",
    "x1 = Conv1D(f,4,dilation_rate=2,padding='causal',activation='relu',kernel_initializer=initializer)(x1)\n",
    "x1 = BatchNormalization()(x1)\n",
    "x1 = Conv1D(f,4,dilation_rate=4,padding='causal',activation='relu',kernel_initializer=initializer)(x1)\n",
    "x1 = BatchNormalization()(x1)\n",
    "x1 = GlobalMaxPooling1D()(x1)\n",
    "\n",
    "\n",
    "x2 = Conv1D(f,2,padding='causal',activation='relu',kernel_initializer=initializer)(inputs)\n",
    "x2 = BatchNormalization()(x2)\n",
    "x2 = Conv1D(f,2,dilation_rate=2,padding='causal',activation='relu',kernel_initializer=initializer)(x2)\n",
    "x2 = BatchNormalization()(x2)\n",
    "x2 = Conv1D(f,2,dilation_rate=4,padding='causal',activation='relu',kernel_initializer=initializer)(x2)\n",
    "x2 = BatchNormalization()(x2)\n",
    "x2 = GlobalMaxPooling1D()(x2)\n",
    "\n",
    "\n",
    "x3 = Conv1D(f,3,padding='causal',activation='relu',kernel_initializer=initializer)(inputs)\n",
    "x3 = BatchNormalization()(x3)\n",
    "x3 = Conv1D(f,3,dilation_rate=2,padding='causal',activation='relu',kernel_initializer=initializer)(x3)\n",
    "x3 = BatchNormalization()(x3)\n",
    "x3 = Conv1D(f,3,dilation_rate=4,padding='causal',activation='relu',kernel_initializer=initializer)(x3)\n",
    "x3 = BatchNormalization()(x3)\n",
    "x3 = GlobalMaxPooling1D()(x3)\n",
    "\n",
    "\n",
    "x4 = Conv1D(f,10,padding='causal',activation='relu',kernel_initializer=initializer)(inputs)\n",
    "x4 = BatchNormalization()(x4)\n",
    "x4 = Conv1D(f,10,dilation_rate=4,padding='causal',activation='relu',kernel_initializer=initializer)(x4)\n",
    "x4 = BatchNormalization()(x4)\n",
    "x4 = Conv1D(f,10,dilation_rate=8,padding='causal',activation='relu',kernel_initializer=initializer)(x4)\n",
    "x4 = BatchNormalization()(x4)\n",
    "x4 = GlobalMaxPooling1D()(x4)\n",
    "\n",
    "\n",
    "x5 = Conv1D(f,20,padding='causal',activation='relu',kernel_initializer=initializer)(inputs)\n",
    "x5 = BatchNormalization()(x5)\n",
    "x5 = Conv1D(f,20,dilation_rate=2,padding='causal',activation='relu',kernel_initializer=initializer)(x5)\n",
    "x5 = BatchNormalization()(x5)\n",
    "x5 = Conv1D(f,20,dilation_rate=8,padding='causal',activation='relu',kernel_initializer=initializer)(x5)\n",
    "x5 = BatchNormalization()(x5)\n",
    "x5 = GlobalMaxPooling1D()(x5)\n",
    "\n",
    "x7 = Conv1D(f,20,padding='causal',activation='relu',kernel_initializer=initializer)(inputs)\n",
    "x7 = BatchNormalization()(x7)\n",
    "x7 = Conv1D(f,20,dilation_rate=2,padding='causal',activation='relu',kernel_initializer=initializer)(x7)\n",
    "x7 = BatchNormalization()(x7)\n",
    "x7 = Conv1D(f,20,dilation_rate=16,padding='causal',activation='relu',kernel_initializer=initializer)(x7)\n",
    "x7 = BatchNormalization()(x7)\n",
    "x7 = GlobalMaxPooling1D()(x7)\n",
    "\n",
    "\n",
    "x6 = Conv1D(f,20,padding='same',activation='relu',kernel_initializer=initializer)(inputs)\n",
    "x6 = BatchNormalization()(x6)\n",
    "x6 = GlobalMaxPooling1D()(x6)\n",
    "\n",
    "\n",
    "con = concatenate([x1,x2,x3,x4,x5,x6])\n",
    "dense = Dense(512,activation='relu')(con)\n",
    "drop1 = Dropout(0.1)(dense)\n",
    "dense2 = Dense(256,activation='relu')(drop1)\n",
    "drop2 = Dropout(0.1)(dense2)\n",
    "dense3 = Dense(2,activation='softmax')(drop2)\n",
    "model = Model(inputs=inputs, outputs=dense3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512, input_dim=inputs, activation='relu'))\n",
    "#model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "# con = concatenate([x1,x2,x3,x4])\n",
    "# dense = Dense(512,activation='relu')(con)\n",
    "# dense = Dense(256#,activation='relu')(dense)\n",
    "# dense2 = Dense(1,activation='sigmoid')(dense)\n",
    "# model = Model(inputs=inputs, outputs=dense2)\n",
    "\n",
    "# optimizer = Adam(lr=1e-5)\n",
    "#model.compile(optimizer=optimizer,loss='mse',metrics=['mse'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from livelossplot import PlotLossesKeras, PlotLossesKerasTF\n",
    "from keras.callbacks import EarlyStopping,ReduceLROnPlateau,ModelCheckpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [EarlyStopping(monitor='val_loss',\n",
    "                       patience=100,\n",
    "                       verbose=1,\n",
    "                       min_delta=1e-10),\n",
    "         ReduceLROnPlateau(monitor='val_loss',\n",
    "                           factor=0.1,\n",
    "                           patience=50,\n",
    "                           verbose=1,\n",
    "                           min_lr=1e-12),\n",
    "         ModelCheckpoint(filepath='new_model.h5',\n",
    "                         monitor='val_loss',\n",
    "                         save_best_only=True,\n",
    "                         mode='min',\n",
    "                         save_weights_only=False),\n",
    "                         PlotLossesKerasTF()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='mse',\n",
    "              optimizer=tf.optimizers.SGD(lr=0.001),\n",
    "              metrics=['mse', 'accuracy', 'binary_crossentropy'])\n",
    "\n",
    "history = model.fit(training_generator, epochs=2000, steps_per_epoch=200, validation_data=validation_generator, validation_steps=100, callbacks=callbacks, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_confmatrix_xys(list_ids, data_path, labels, dim, num_classes):\n",
    "    X = np.zeros((len(list_ids), *dim))#, self.n_channels))\n",
    "    y = np.zeros(len(list_ids), dtype=int)\n",
    "    for i, ID in enumerate(list_ids):\n",
    "            # Store sample\n",
    "            out_array = np.load(data_path + ID + '.npy')\n",
    "            out_array = out_array[:dim[0], :dim[1]]\n",
    "            out_array_scaled = out_array[:-1] - out_array[1:]\n",
    "            #out_array_scaled = out_array/2048.0\n",
    "            #out_array = normalize(out_array)\n",
    "            X[i, 0:len(out_array_scaled), 0:2] = out_array_scaled\n",
    "\n",
    "            # Store class\n",
    "            y[i] = labels[ID]\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = get_confmatrix_xys(list_ids=partition['validation'], data_path='/Users/nelsschimek/Documents/nancelab/diff_predictor/data/cortex_mpt_data_nonans/', labels=validation_labels, dim=(651,2), num_classes=2)\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "Y_pred = model.predict(x, )\n",
    "y_pred = np.argmax(Y_pred, axis=1)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(Y_pred[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "cm = confusion_matrix(y, y_pred)\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Rot', 'NT'])\n",
    "\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import scale\n",
    "output = scale()\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_y = np.array(df[df['Track_ID']== 1][['X', 'Y']])\n",
    "x_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm = np.linalg.norm(x_y, 1)\n",
    "im = x_y/norm\n",
    "plt.figure(figsize=(10,500))\n",
    "plt.imshow(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_y[:2,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_array(A):\n",
    "    return (A-np.min(A))/(np.max(A) - np.min(A))\n",
    "\n",
    "output = scale_array(x_y[:19,:])\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "%matplotlib inline\n",
    "im = np.array(output).astype('uint8')\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(im, cmap='Greys')\n",
    "# grayImage = cv2.cvtColor(uint_img, cv2.COLOR_GRAY2BGR)\n",
    "# cv2.imshow(mat=grayImage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def traj_generator(batch_size, datapath):\n",
    "    out = np.zeros([batch_size, 651, 2]) #initialize trajectory array\n",
    "    label = np.zeros([batch_size, 1]) #initialize label array\n",
    "    batch_count = 0\n",
    "    while True:\n",
    "        df = pd.read_csv(datapath)\n",
    "        for i in range(batch_size):\n",
    "            batch_count += 1\n",
    "            x_y = np.array(df[df['Track_ID']==i][['X', 'Y']])\n",
    "            out[i,:,:] = x_y\n",
    "            label[i] = 1\n",
    "        yield out, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = Input((651, 2))\n",
    "initializer = 'random_normal'\n",
    "f = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = Conv1D(f,4,padding='causal',activation='relu',kernel_initializer=initializer)(inputs)\n",
    "x1 = BatchNormalization()(x1)\n",
    "x1 = Conv1D(f,4,dilation_rate=2,padding='causal',activation='relu',kernel_initializer=initializer)(x1)\n",
    "x1 = BatchNormalization()(x1)\n",
    "x1 = Conv1D(f,4,dilation_rate=4,padding='causal',activation='relu',kernel_initializer=initializer)(x1)\n",
    "x1 = BatchNormalization()(x1)\n",
    "x1 = GlobalMaxPooling1D()(x1)\n",
    "\n",
    "\n",
    "x2 = Conv1D(f,2,padding='causal',activation='relu',kernel_initializer=initializer)(inputs)\n",
    "x2 = BatchNormalization()(x2)\n",
    "x2 = Conv1D(f,2,dilation_rate=2,padding='causal',activation='relu',kernel_initializer=initializer)(x2)\n",
    "x2 = BatchNormalization()(x2)\n",
    "x2 = Conv1D(f,2,dilation_rate=4,padding='causal',activation='relu',kernel_initializer=initializer)(x2)\n",
    "x2 = BatchNormalization()(x2)\n",
    "x2 = GlobalMaxPooling1D()(x2)\n",
    "\n",
    "\n",
    "x3 = Conv1D(f,3,padding='causal',activation='relu',kernel_initializer=initializer)(inputs)\n",
    "x3 = BatchNormalization()(x3)\n",
    "x3 = Conv1D(f,3,dilation_rate=2,padding='causal',activation='relu',kernel_initializer=initializer)(x3)\n",
    "x3 = BatchNormalization()(x3)\n",
    "x3 = Conv1D(f,3,dilation_rate=4,padding='causal',activation='relu',kernel_initializer=initializer)(x3)\n",
    "x3 = BatchNormalization()(x3)\n",
    "x3 = GlobalMaxPooling1D()(x3)\n",
    "\n",
    "\n",
    "x4 = Conv1D(f,10,padding='causal',activation='relu',kernel_initializer=initializer)(inputs)\n",
    "x4 = BatchNormalization()(x4)\n",
    "x4 = Conv1D(f,10,dilation_rate=5,padding='causal',activation='relu',kernel_initializer=initializer)(x4)\n",
    "x4 = BatchNormalization()(x4)\n",
    "x4 = Conv1D(f,10,dilation_rate=10,padding='causal',activation='relu',kernel_initializer=initializer)(x4)\n",
    "x4 = BatchNormalization()(x4)\n",
    "x4 = GlobalMaxPooling1D()(x4)\n",
    "\n",
    "con = concatenate([x1,x2,x3,x4])\n",
    "dense = Dense(512,activation='relu')(con)\n",
    "dense = Dense(256,activation='relu')(dense)\n",
    "dense2 = Dense(1,activation='sigmoid')(dense)\n",
    "model = Model(inputs=inputs, outputs=dense2)\n",
    "\n",
    "optimizer = Adam(lr=1e-5)\n",
    "model.compile(optimizer=optimizer,loss='mse',metrics=['mse', 'accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(16, input_dim=1, activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(1, activation='linear'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='mean_absolute_error',\n",
    "              optimizer=Adam(),\n",
    "              metrics=['mean_squared_error'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='mean_absolute_error',\n",
    "              optimizer=Adam(),\n",
    "              metrics=['mean_squared_error', 'accuracy'])\n",
    "\n",
    "model.fit(training_generator, epochs=2, validation_data=validation_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
  },
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
